# VideosCommentsSpotter/agent.py

import os
import sys
import json
import time
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
log_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'resource', 'VideosCommentsSpotter.log')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- å¯¼å…¥ç­–ç•¥ï¼šå…ˆå¯¼å…¥æ‰€æœ‰ä¸éœ€è¦MediaCrawlerçš„æ¨¡å— ---

# 1. ç¡®ä¿å½“å‰ç›®å½•åœ¨sys.pathä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# 2. å¯¼å…¥VideosCommentsSpotterè‡ªå·±çš„æ¨¡å—ï¼ˆä¸åŒ…æ‹¬çˆ¬è™«æ¨¡å—ï¼‰
from llm import LLMClient
from utils.config import LLM_CONFIG, OUTPUT_DIRECTORY
from prompts.prompts import VCS_KEYWORD_PROMPT, VCS_ANALYSIS_PROMPT

# 3. å¯¼å…¥çˆ¬è™«æ¨¡å—çš„ç‰¹æ®Šå¤„ç†
# 3.1 è·å–çˆ¬è™«æ¨¡å—çš„å®Œæ•´è·¯å¾„
crawler_file_path = os.path.join(current_dir, 'tools', 'videoscomments_crawler.py')

# 3.2 æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(crawler_file_path):
    raise FileNotFoundError(f"çˆ¬è™«æ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨: {crawler_file_path}")

# 3.3 ä½¿ç”¨æ›´é«˜æ•ˆçš„å¯¼å…¥æ–¹å¼
VideoCommentSpotter = None

try:
    # ç›´æ¥å¯¼å…¥çˆ¬è™«ç±»ï¼Œæé«˜å¯¼å…¥æ•ˆç‡
    from tools.videoscomments_crawler import VideoCommentSpotter
    # åªåœ¨å®é™…è¿è¡Œæ—¶æ‰è¾“å‡ºæ—¥å¿—ï¼Œå¯¼å…¥æ—¶ä¸è¾“å‡º
    # logger.info("âœ… æˆåŠŸä½¿ç”¨ç›´æ¥å¯¼å…¥æ–¹å¼å¯¼å…¥VideoCommentSpotter")
except ImportError:
    # å¦‚æœç›´æ¥å¯¼å…¥å¤±è´¥ï¼Œå°è¯•åŠ¨æ€å¯¼å…¥
    import importlib.util
    logger.warning("âš ï¸  ç›´æ¥å¯¼å…¥å¤±è´¥ï¼Œå°è¯•åŠ¨æ€å¯¼å…¥")
    try:
        # åˆ›å»ºä¸€ä¸ªæ¨¡å—è§„èŒƒ
        spec = importlib.util.spec_from_file_location("videoscomments_crawler", crawler_file_path)
        
        # åˆ›å»ºä¸€ä¸ªæ¨¡å—å¯¹è±¡
        videoscomments_crawler = importlib.util.module_from_spec(spec)
        
        # å°†æ¨¡å—æ·»åŠ åˆ°sys.modulesä¸­
        sys.modules["videoscomments_crawler"] = videoscomments_crawler
        
        # æ‰§è¡Œæ¨¡å—çš„ä»£ç 
        spec.loader.exec_module(videoscomments_crawler)
        
        # ä»æ¨¡å—ä¸­è·å–VideoCommentSpotterç±»
        VideoCommentSpotter = videoscomments_crawler.VideoCommentSpotter
        
        # åªåœ¨å®é™…è¿è¡Œæ—¶æ‰è¾“å‡ºæ—¥å¿—ï¼Œå¯¼å…¥æ—¶ä¸è¾“å‡º
        # logger.info("âœ… æˆåŠŸä½¿ç”¨åŠ¨æ€å¯¼å…¥æ–¹å¼å¯¼å…¥VideoCommentSpotter")
        
    except Exception as e:
        logger.error(f"æ— æ³•åŠ¨æ€å¯¼å…¥VideoCommentSpotter: {e}")
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        import traceback
        traceback.print_exc()
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„VideoCommentSpotterç±»ï¼Œä»¥ä¾¿ä»£ç å¯ä»¥ç»§ç»­è¿è¡Œ
        class MockVideoCommentSpotter:
            def __init__(self, platform="dy"):
                self.platform = platform
                self.crawlers = {}
            
            def search_multiple(self, keywords, max_count=20, max_retries=3, max_concurrency=3, platform_config=None):
                logger.warning(f"æ¨¡æ‹Ÿçˆ¬è™«: åœ¨{self.platform}å¹³å°ä¸Šæœç´¢å…³é”®è¯ {keywords}ï¼Œä½†å®é™…çˆ¬è™«æœªå¯¼å…¥")
                return {
                    "results": [],
                    "total_items": 0,
                    "total_comments": 0,
                    "platform": self.platform,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                }
            
            def search(self, keyword, max_count=None, max_retries=None, enable_get_comments=None, platform_config=None):
                logger.warning(f"æ¨¡æ‹Ÿçˆ¬è™«: åœ¨{self.platform}å¹³å°ä¸Šæœç´¢å…³é”®è¯ {keyword}ï¼Œä½†å®é™…çˆ¬è™«æœªå¯¼å…¥")
                return {
                    "keyword": keyword,
                    "platform": self.platform,
                    "error": "å®é™…çˆ¬è™«æœªå¯¼å…¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿçˆ¬è™«",
                    "items": [],
                    "total_items": 0,
                    "total_comments": 0,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                }
        
        VideoCommentSpotter = MockVideoCommentSpotter
        logger.warning("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿçš„VideoCommentSpotterç±»ï¼Œçˆ¬è™«åŠŸèƒ½å°†ä¸å¯ç”¨")


class VideosCommentsSpotterAgent:
    """
    Videos & Comments Spotter Agent (ä¾¦å¯Ÿå…µ/åˆ†æè€…):
    1. æ¥æ”¶Risk Analyzerä¼ æ¥çš„å±é™©è¯é¢˜
    2. ä½¿ç”¨LLMåˆ†æè¯é¢˜ç”Ÿæˆå…³é”®è¯
    3. è°ƒç”¨MediaCrawlerçˆ¬å–ç›¸å…³è§†é¢‘å’Œè¯„è®º
    4. åˆ†æçˆ¬å–å†…å®¹ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    5. è¿”å›æŠ¥å‘Šç»™Risk Analyzer
    """

    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client
        self.output_dir = Path(OUTPUT_DIRECTORY)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.memory_file = self.output_dir / "vcs_memory.json"
        self.memory: List[Dict] = self._load_memory()
        
    def _load_memory(self) -> List[Dict]:
        """åŠ è½½å†å²è®°å¿†"""
        if self.memory_file.exists():
            with open(self.memory_file, "r", encoding="utf-8") as f:
                try:
                    # åªåŠ è½½æœ€è¿‘200æ¡è®°å¿†
                    return json.load(f)[-200:]
                except json.JSONDecodeError:
                    return []
        return []
    
    def _save_memory(self):
        """ä¿å­˜å†å²è®°å¿†"""
        with open(self.memory_file, "w", encoding="utf-8") as f:
            # åªä¿å­˜æœ€è¿‘1000æ¡è®°å¿†
            json.dump(self.memory[-1000:], f, ensure_ascii=False, indent=2)
    
    def generate_keywords(self, risk_topic: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMåˆ†æå±é™©è¯é¢˜ï¼Œç”Ÿæˆé€‚åˆçˆ¬å–çš„å…³é”®è¯å’Œçˆ¬å–å‚æ•°
        
        Args:
            risk_topic: æ¥è‡ªRisk Analyzerçš„å±é™©è¯é¢˜
            
        Returns:
            åŒ…å«å…³é”®è¯ã€çˆ¬å–å‚æ•°çš„é…ç½®
        """
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å¤æ‚è¯é¢˜ä¸­æå–å…³é”®ä¿¡æ¯å¹¶åˆ¶å®šæœç´¢ç­–ç•¥ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºJSONç»“æœã€‚"
        
        # è·å–åŸè¯é¢˜æ–‡æœ¬ï¼Œç”¨äºåç»­å…³é”®è¯éªŒè¯
        original_topic = risk_topic.get('topic', '').strip() or 'æœªå‘½åè¯é¢˜'
        
        # ä¿®æ”¹ç”¨æˆ·æç¤ºï¼Œè¦æ±‚ç”Ÿæˆçš„å…³é”®è¯æ›´åŠ ä¸¥æ ¼åœ°åŸºäºåŸè¯é¢˜
        user_prompt = VCS_KEYWORD_PROMPT
        user_prompt = user_prompt.replace('{risk_topic}', json.dumps(risk_topic, ensure_ascii=False))
        
        # å¢å¼ºæç¤ºè¯ï¼Œè¦æ±‚å…³é”®è¯æ›´åŠ ç²¾å‡†ï¼ŒåŒæ—¶æå‡é¢„è­¦é˜ˆå€¼
        user_prompt += "\n\nç‰¹åˆ«è¦æ±‚ï¼š"
        user_prompt += "1. ç”Ÿæˆçš„å…³é”®è¯å¿…é¡»ç›´æ¥æ¥è‡ªåŸè¯é¢˜ï¼Œæˆ–è€…æ˜¯åŸè¯é¢˜çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†"
        user_prompt += "2. åªç”Ÿæˆä¸åŸè¯é¢˜é«˜åº¦ç›¸å…³çš„å…³é”®è¯ï¼Œé¿å…ç”Ÿæˆæ— å…³æˆ–å¼±ç›¸å…³çš„æ‰©å±•å…³é”®è¯"
        user_prompt += "3. å…³é”®è¯æ•°é‡æ§åˆ¶åœ¨3-5ä¸ªï¼Œç¡®ä¿æ¯ä¸ªå…³é”®è¯éƒ½å…·æœ‰é«˜åº¦ç›¸å…³æ€§"
        user_prompt += f"4. åŸè¯é¢˜æ˜¯ '{original_topic}'ï¼Œè¯·ç¡®ä¿æ‰€æœ‰å…³é”®è¯éƒ½ç´§å¯†å›´ç»•è¿™ä¸ªè¯é¢˜"
        user_prompt += "5. ç”Ÿæˆçš„å…³é”®è¯åº”ä¼˜å…ˆè€ƒè™‘é‚£äº›å¯èƒ½åŒ…å«è´Ÿé¢èˆ†æƒ…çš„è¯æ±‡"
        user_prompt += "6. æé«˜é£é™©é¢„è­¦é˜ˆå€¼ï¼Œåªå…³æ³¨çœŸæ­£å¯èƒ½å­˜åœ¨é£é™©çš„å†…å®¹"
        
        try:
            response = self.llm.invoke(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                json_mode=True,
                temperature=0.1  # å¤§å¹…é™ä½æ¸©åº¦ï¼Œç¡®ä¿å…³é”®è¯ä¸¥æ ¼åŸºäºåŸè¯é¢˜ï¼Œå‡å°‘å¹»è§‰
            )
            
            result = json.loads(response)
            
            # éªŒè¯ç»“æœæ ¼å¼
            if isinstance(result, dict) and 'keywords_config' in result:
                # å¯¹ç”Ÿæˆçš„å…³é”®è¯è¿›è¡Œè¿‡æ»¤å’ŒéªŒè¯ï¼ˆæ›´ä¸¥æ ¼çš„éªŒè¯ï¼‰
                filtered_keywords = []
                for kw_config in result['keywords_config']:
                    keyword = kw_config.get('keyword', '').strip()
                    
                    # 1. æ£€æŸ¥å…³é”®è¯æ˜¯å¦ä¸ºç©º
                    if not keyword:
                        logger.warning(f"è¿‡æ»¤æ‰ç©ºå…³é”®è¯")
                        continue
                    
                    # 2. æ£€æŸ¥å…³é”®è¯é•¿åº¦ï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿éƒ½ä¸åˆé€‚ï¼‰
                    if len(keyword) < 2 or len(keyword) > 20:
                        logger.warning(f"è¿‡æ»¤æ‰é•¿åº¦ä¸åˆé€‚çš„å…³é”®è¯: {keyword} (é•¿åº¦: {len(keyword)})")
                        continue
                    
                    # 3. æ£€æŸ¥å…³é”®è¯æ˜¯å¦ä¸åŸè¯é¢˜é«˜åº¦ç›¸å…³
                    if self._is_keyword_relevant(keyword, original_topic):
                        # 4. éªŒè¯çˆ¬å–å‚æ•°æ˜¯å¦åˆç†
                        max_video_count = kw_config.get('max_video_count', 5)
                        max_comment_count = kw_config.get('max_comment_count', 15)
                        
                        # é™åˆ¶çˆ¬å–æ•°é‡ï¼Œé¿å…è¿‡åº¦çˆ¬å–
                        max_video_count = min(max_video_count, 5)
                        max_comment_count = min(max_comment_count, 15)
                        
                        kw_config['max_video_count'] = max_video_count
                        kw_config['max_comment_count'] = max_comment_count
                        filtered_keywords.append(kw_config)
                        logger.info(f"âœ… éªŒè¯é€šè¿‡çš„å…³é”®è¯: {keyword}")
                    else:
                        logger.warning(f"âŒ è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å…³é”®è¯: {keyword} (åŸè¯é¢˜: {original_topic})")
                
                # å¦‚æœè¿‡æ»¤åæ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨åŸè¯é¢˜çš„æ ¸å¿ƒéƒ¨åˆ†ä½œä¸ºé»˜è®¤å…³é”®è¯
                if not filtered_keywords:
                    # æå–åŸè¯é¢˜çš„æ ¸å¿ƒéƒ¨åˆ†ï¼ˆå»é™¤å¸¸è§åœç”¨è¯ï¼‰
                    stop_words = ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'ä¸', 'åŠ', 'æˆ–', 'ä½†', 'è€Œ', 'ç­‰', 'é—®é¢˜', 'äº‹ä»¶', 'æƒ…å†µ']
                    topic_words = [w for w in original_topic if w not in stop_words and len(w) >= 2]
                    if topic_words:
                        # å–å‰3ä¸ªæ ¸å¿ƒè¯ä½œä¸ºå…³é”®è¯
                        core_keyword = ''.join(topic_words[:3])
                    else:
                        core_keyword = original_topic[:10]  # å¦‚æœæ— æ³•æå–ï¼Œä½¿ç”¨å‰10ä¸ªå­—ç¬¦
                    
                    logger.warning(f"æ‰€æœ‰ç”Ÿæˆçš„å…³é”®è¯éƒ½è¢«è¿‡æ»¤ï¼Œä½¿ç”¨åŸè¯é¢˜æ ¸å¿ƒéƒ¨åˆ†ä½œä¸ºé»˜è®¤å…³é”®è¯: {core_keyword}")
                    filtered_keywords = [{'keyword': core_keyword, 'max_video_count': 5, 'max_comment_count': 15}]
                
                # å°†è¿‡æ»¤åçš„å…³é”®è¯æ˜ å°„åˆ°å†…éƒ¨é…ç½®æ ¼å¼
                crawl_config = {
                    'keywords_config': filtered_keywords,
                    'keywords': [kw['keyword'] for kw in filtered_keywords],
                    'platforms': ['dy'],  # åªçˆ¬å–æŠ–éŸ³å¹³å°
                    'retries': result.get('max_retries', 3)
                }
                
                # æ‰“å°å…³é”®è¯åŠå‚æ•°æŠ¥å‘Š
                logger.info(f"\nğŸ”‘ å…³é”®è¯åŠå‚æ•°æŠ¥å‘Š")
                logger.info(f"ç›®æ ‡è¯é¢˜: {original_topic}")
                logger.info(f"ç”Ÿæˆå…³é”®è¯æ•°é‡: {len(crawl_config['keywords'])}")
                logger.info(f"ç›®æ ‡å¹³å°: {'æŠ–éŸ³'}")
                logger.info(f"çˆ¬å–é‡è¯•æ¬¡æ•°: {crawl_config['retries']}")
                
                # æ‰“å°æ¯ä¸ªå…³é”®è¯çš„è¯¦ç»†é…ç½®
                logger.info(f"\nğŸ“‹ å…³é”®è¯é…ç½®è¯¦æƒ…:")
                for i, kw_config in enumerate(crawl_config['keywords_config']):
                    logger.info(f"{i+1}. å…³é”®è¯: {kw_config['keyword']}")
                    logger.info(f"    çˆ¬å–è§†é¢‘æ•°é‡: {kw_config['max_video_count']}")
                    logger.info(f"    æ¯ä¸ªè§†é¢‘çˆ¬å–è¯„è®ºæ•°é‡: {kw_config['max_comment_count']}")
                
                return crawl_config
            else:
                # è¿”å›é»˜è®¤é…ç½®
                base_keyword = original_topic
                return {
                    'keywords_config': [{'keyword': base_keyword, 'max_video_count': 8, 'max_comment_count': 20}],
                    'keywords': [base_keyword],
                    'platforms': ['dy'],  # åªçˆ¬å–æŠ–éŸ³å¹³å°
                    'retries': 3
                }
        except Exception as e:
            logger.error(f"ç”Ÿæˆå…³é”®è¯å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€å…³é”®è¯ä½œä¸ºåå¤‡
            base_keyword = original_topic
            return {
                'keywords_config': [{'keyword': base_keyword, 'max_video_count': 8, 'max_comment_count': 20}],
                'keywords': [base_keyword],
                'platforms': ['dy'],  # åªçˆ¬å–æŠ–éŸ³å¹³å°
                'retries': 3
            }
    
    def _is_keyword_relevant(self, keyword: str, original_topic: str) -> bool:
        """
        éªŒè¯å…³é”®è¯æ˜¯å¦ä¸åŸè¯é¢˜é«˜åº¦ç›¸å…³ï¼ˆæ›´ä¸¥æ ¼çš„éªŒè¯ï¼‰
        
        Args:
            keyword: ç”Ÿæˆçš„å…³é”®è¯
            original_topic: åŸè¯é¢˜
            
        Returns:
            bool: å…³é”®è¯æ˜¯å¦ç›¸å…³
        """
        # å»é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ï¼Œä¾¿äºæ¯”è¾ƒ
        keyword_clean = keyword.strip().replace(' ', '').replace('ï¼Œ', '').replace(',', '')
        topic_clean = original_topic.strip().replace(' ', '').replace('ï¼Œ', '').replace(',', '')
        
        # 1. å…³é”®è¯å®Œå…¨ç­‰äºåŸè¯é¢˜ï¼ˆæˆ–åŒ…å«åŸè¯é¢˜çš„æ ¸å¿ƒéƒ¨åˆ†ï¼‰
        if keyword_clean == topic_clean or keyword_clean in topic_clean:
            return True
        
        # 2. åŸè¯é¢˜åŒ…å«å…³é”®è¯ï¼ˆå…³é”®è¯æ˜¯åŸè¯é¢˜çš„å­ä¸²ï¼‰
        if keyword_clean in topic_clean:
            return True
        
        # 3. æ£€æŸ¥å…³é”®è¯æ˜¯å¦åŒ…å«åŸè¯é¢˜ä¸­çš„æ ¸å¿ƒå­—ç¬¦ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦åŒ¹é…ï¼‰
        # æå–åŸè¯é¢˜ä¸­çš„æ ¸å¿ƒå­—ç¬¦ï¼ˆé•¿åº¦>=2çš„å­—ç¬¦åºåˆ—ï¼‰
        # å¯¹äºä¸­æ–‡ï¼Œæˆ‘ä»¬æ£€æŸ¥å…³é”®è¯æ˜¯å¦åŒ…å«åŸè¯é¢˜ä¸­çš„å…³é”®å­—ç¬¦åºåˆ—
        if len(original_topic) >= 2 and len(keyword) >= 2:
            # æ£€æŸ¥å…³é”®è¯ä¸­æ˜¯å¦åŒ…å«åŸè¯é¢˜ä¸­çš„2-4å­—ç¬¦åºåˆ—
            for i in range(len(original_topic) - 1):
                for length in [2, 3, 4]:
                    if i + length <= len(original_topic):
                        topic_substring = original_topic[i:i+length]
                        if topic_substring in keyword:
                            return True
        
        # 4. æ£€æŸ¥å…³é”®è¯æ˜¯å¦åŒ…å«åŸè¯é¢˜ä¸­çš„å…³é”®å­—ç¬¦ï¼ˆè‡³å°‘åŒ…å«3ä¸ªè¿ç»­å­—ç¬¦ï¼‰
        if len(keyword_clean) >= 3 and len(topic_clean) >= 3:
            for i in range(len(topic_clean) - 2):
                substring = topic_clean[i:i+3]
                if substring in keyword_clean:
                    return True
        
        # 5. è¿‡æ»¤æ‰æ˜æ˜¾çš„æ— å…³è¯æ±‡
        irrelevant_words = ['æµ‹è¯•', 'ç¤ºä¾‹', 'ä¾‹å­', 'test', 'example', 'é—®é¢˜', 'äº‹ä»¶', 'æƒ…å†µ', 'äº‹æƒ…']
        if any(irr in keyword_clean for irr in irrelevant_words):
            return False
        
        # å¦‚æœéƒ½ä¸æ»¡è¶³ï¼Œåˆ™è®¤ä¸ºä¸ç›¸å…³
        return False
    
    def analyze_content(self, crawl_results: Dict[str, Any], risk_topic: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMåˆ†æçˆ¬å–çš„è§†é¢‘å’Œè¯„è®ºå†…å®¹
        
        Args:
            crawl_results: çˆ¬å–ç»“æœ
            risk_topic: åŸå§‹å±é™©è¯é¢˜
            
        Returns:
            åˆ†ææŠ¥å‘Š
        """
        logger.info(f"å¼€å§‹åˆ†æå†…å®¹ï¼Œä¸»é¢˜: {risk_topic.get('topic', 'æœªå‘½å')}ï¼Œçˆ¬å–å¹³å°æ•°: {len(crawl_results.get('platform_results', {})) if crawl_results else 0}")
        
        try:
            # é¢„å¤„ç†çˆ¬å–ç»“æœï¼Œæå–å…³é”®ä¿¡æ¯
            logger.debug(f"é¢„å¤„ç†çˆ¬å–æ•°æ®ï¼ŒåŸå§‹æ•°æ®å¤§å°: {len(json.dumps(crawl_results)) if crawl_results else 0} å­—èŠ‚")
            processed_data = self._preprocess_crawl_data(crawl_results)
            logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå¤„ç†åçš„å†…å®¹æ•°: {processed_data.get('total_content_count', 0)}")
            
            # å‡†å¤‡ç³»ç»Ÿæç¤ºè¯
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ†æƒ…åˆ†æå¸ˆï¼Œè¯·åŸºäºçˆ¬å–çš„å†…å®¹ï¼Œæ·±å…¥åˆ†æè¯é¢˜çš„å‘å±•è¶‹åŠ¿ã€å…¬ä¼—æƒ…ç»ªå’Œæ½œåœ¨é£é™©ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºJSONç»“æœã€‚"
            
            # å‡†å¤‡ç”¨æˆ·æç¤º
            logger.debug("ç”Ÿæˆåˆ†ææç¤ºè¯")
            user_prompt = self._generate_analysis_prompt(risk_topic, processed_data)
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            logger.info("è°ƒç”¨LLMè¿›è¡Œå†…å®¹åˆ†æ")
            start_time = time.time()
            response = self.llm.invoke(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                json_mode=True,
                temperature=0.1  # å¤§å¹…é™ä½æ¸©åº¦ï¼Œç¡®ä¿åˆ†æä¸¥æ ¼åŸºäºäº‹å®ï¼Œå‡å°‘å¹»è§‰
            )
            end_time = time.time()
            logger.info(f"LLMåˆ†æå®Œæˆï¼Œè€—æ—¶: {(end_time - start_time):.2f} ç§’")
            
            # è§£æå’ŒéªŒè¯å“åº”
            logger.debug("è§£æå¹¶éªŒè¯LLMè¿”å›çš„æŠ¥å‘Š")
            analysis = self._parse_and_validate_report(response)
            
            # æ·»åŠ å…ƒæ•°æ®
            analysis.update({
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'source_topic': risk_topic.get('topic', ''),
                'data_count': processed_data.get('total_content_count', 0),
                'comment_count': processed_data.get('total_comment_count', 0),
                'confidence_score': analysis.get('confidence_score', 0.5)
            })
            
            logger.info(f"å†…å®¹åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼Œé£é™©ç­‰çº§: {analysis.get('risk_assessment', {}).get('level', 'unknown')}")
            return analysis
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"åˆ†æå†…å®¹å¤±è´¥: {error_msg}", exc_info=True)
            # è¿”å›é”™è¯¯æŠ¥å‘Š
            error_report = self._generate_error_report(risk_topic, error_msg)
            logger.warning(f"ç”Ÿæˆé”™è¯¯æŠ¥å‘Šï¼Œä¸»é¢˜: {risk_topic.get('topic', 'æœªå‘½å')}")
            return error_report
    
    def _preprocess_crawl_data(self, crawl_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        é¢„å¤„ç†çˆ¬å–ç»“æœï¼Œæå–å…³é”®ä¿¡æ¯
        
        Args:
            crawl_results: åŸå§‹çˆ¬å–ç»“æœ
            
        Returns:
            å¤„ç†åçš„ç»“æ„åŒ–æ•°æ®
        """
        processed = {
            "platforms": list(crawl_results.get("platform_results", {}).keys()),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "items": [],
            "total_content_count": 0,
            "total_comment_count": 0
        }
        
        # ä»å„å¹³å°æå–å†…å®¹
        platform_results = crawl_results.get("platform_results", {})
        for platform, data in platform_results.items():
            platform_items = data.get("items", [])
            for item in platform_items:
                processed_item = {
                    "platform": platform,
                    "title": item.get("title", ""),
                    "content": item.get("content", ""),
                    "author": item.get("author", "anonymous"),
                    "likes": item.get("likes_count", 0),
                    "comments": item.get("comments", [])[:20]  # é™åˆ¶è¯„è®ºæ•°é‡ä»¥é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                }
                processed["items"].append(processed_item)
                processed["total_comment_count"] += len(item.get("comments", []))
        
        processed["total_content_count"] = len(processed["items"])
        
        # é™åˆ¶å†…å®¹æ€»é‡
        max_items = 10  # æœ€å¤šåˆ†æ10ä¸ªå†…å®¹é¡¹
        processed["items"] = processed["items"][:max_items]
        
        return processed
    
    def _generate_analysis_prompt(self, risk_topic: Dict[str, Any], processed_data: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆåˆ†ææç¤ºè¯ï¼Œä¼˜åŒ–ä»¥æé«˜é¢„è­¦é˜ˆå€¼
        
        Args:
            risk_topic: å±é™©è¯é¢˜
            processed_data: å¤„ç†åçš„çˆ¬å–æ•°æ®
            
        Returns:
            æ ¼å¼åŒ–çš„åˆ†ææç¤ºè¯
        """
        topic = risk_topic.get('topic', '')
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹å…³äºè¯é¢˜'{topic}'çš„çˆ¬å–å†…å®¹ï¼š
        å¹³å°: {', '.join(processed_data['platforms'])}
        æŠ“å–æ—¶é—´: {processed_data['timestamp']}
        å†…å®¹æ•°é‡: {processed_data['total_content_count']}
        è¯„è®ºæ•°é‡: {processed_data['total_comment_count']}
        
        è¯é¢˜è¯¦æƒ…: {json.dumps(risk_topic, ensure_ascii=False)}
        
        è¯¦ç»†å†…å®¹:
        {json.dumps(processed_data['items'], ensure_ascii=False, indent=2)}
        
        è¯·ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„åˆ†ææŠ¥å‘Šï¼Œé‡ç‚¹å…³æ³¨ï¼š
        1. æ‘˜è¦ï¼šç®€è¦æ¦‚æ‹¬çˆ¬å–å†…å®¹çš„ä¸»è¦å†…å®¹å’ŒèŒƒå›´
        2. å…³é”®å‘ç°ï¼šè¯†åˆ«å‡ºçš„é‡è¦ä¿¡æ¯ã€çƒ­ç‚¹è®¨è®ºå’Œè¶‹åŠ¿
        3. æƒ…ç»ªåˆ†æï¼šåˆ†ææ•´ä½“æƒ…ç»ªå€¾å‘ï¼ŒåŒ…æ‹¬æ­£é¢/è´Ÿé¢/ä¸­æ€§æ¯”ä¾‹
        4. é£é™©è¯„ä¼°ï¼š
           - é£é™©ç­‰çº§ï¼šåªèƒ½æ˜¯"ä½"ã€"ä¸­"ã€"é«˜"ã€"æé«˜"ä¸­çš„ä¸€ä¸ª
           - é£é™©å› ç´ ï¼šè¯¦ç»†è¯´æ˜é£é™©äº§ç”Ÿçš„åŸå› 
           - æé«˜é¢„è­¦é˜ˆå€¼ï¼Œåªå…³æ³¨çœŸæ­£å¯èƒ½å­˜åœ¨é£é™©çš„å†…å®¹
           - ä¸¥æ ¼åŒºåˆ†äº‹å®å’Œæ¨æµ‹ï¼Œåªå°†æœ‰æ˜ç¡®ä¾æ®çš„å†…å®¹æ ‡è®°ä¸ºé£é™©
        5. è¶‹åŠ¿é¢„æµ‹ï¼šé¢„æµ‹è¯é¢˜çš„å‘å±•è¶‹åŠ¿
        6. å»ºè®®æªæ–½ï¼šåŸºäºåˆ†æç»“æœæä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®
        7. ç½®ä¿¡åº¦è¯„åˆ†ï¼šå¯¹åˆ†æç»“æœçš„å¯ä¿¡åº¦è¿›è¡Œ0-1çš„è¯„åˆ†ï¼Œåˆ†æ•°è¶Šé«˜è¡¨ç¤ºå¯ä¿¡åº¦è¶Šé«˜
        
        ç‰¹åˆ«è¦æ±‚ï¼š
        - æé«˜é£é™©é¢„è­¦é˜ˆå€¼ï¼Œé¿å…å°†æ­£å¸¸å†…å®¹è¯¯åˆ¤ä¸ºé£é™©
        - ä¸¥æ ¼åŸºäºäº‹å®è¿›è¡Œåˆ†æï¼Œä¸å¾—æ·»åŠ ä»»ä½•æœªæåŠçš„ä¿¡æ¯
        - åªå°†æœ‰æ˜ç¡®è¯æ®æ”¯æŒçš„å†…å®¹æ ‡è®°ä¸ºé£é™©
        - é£é™©è¯„ä¼°åº”ä¸¥è°¨ï¼Œé¿å…è¿‡åº¦æ•æ„Ÿ
        
        è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
        {{"summary": "", "key_findings": [], "sentiment_analysis": {{"positive": 0, "neutral": 0, "negative": 0}}, "risk_assessment": {{"level": "", "factors": []}}, "trend_prediction": "", "recommendations": [], "confidence_score": 0.5}}
        """
        return prompt
    
    def _parse_and_validate_report(self, response: Any) -> Dict[str, Any]:
        """
        è§£æå¹¶éªŒè¯åˆ†ææŠ¥å‘Š
        
        Args:
            response: LLMå“åº”
            
        Returns:
            æ ‡å‡†åŒ–çš„åˆ†ææŠ¥å‘Š
        """
        # é»˜è®¤æŠ¥å‘Šç»“æ„
        default_report = {
            'summary': 'åˆ†ææŠ¥å‘Šæ‘˜è¦',
            'key_findings': [],
            'sentiment_analysis': {'positive': 0, 'neutral': 0, 'negative': 0},
            'risk_assessment': {'level': 'medium', 'factors': []},
            'trend_prediction': 'æš‚æ— è¶‹åŠ¿é¢„æµ‹',
            'recommendations': [],
            'confidence_score': 0.5
        }
        
        # è§£æå“åº”
        try:
            if isinstance(response, dict):
                report = response
            else:
                report = json.loads(response)
            
            # éªŒè¯å’Œæ ‡å‡†åŒ–æŠ¥å‘Šç»“æ„
            validated_report = default_report.copy()
            
            # æå–å’ŒéªŒè¯å„ä¸ªå­—æ®µ
            validated_report["summary"] = str(report.get("summary", ""))
            
            # ç¡®ä¿key_findingsæ˜¯åˆ—è¡¨
            key_findings = report.get("key_findings", [])
            validated_report["key_findings"] = list(key_findings) if isinstance(key_findings, (list, tuple)) else [str(key_findings)]
            
            # ç¡®ä¿sentiment_analysisæ˜¯å­—å…¸å¹¶åŒ…å«å¿…è¦å­—æ®µ
            sentiment = report.get("sentiment_analysis", {})
            if isinstance(sentiment, dict):
                validated_report["sentiment_analysis"] = {
                    "positive": float(sentiment.get("positive", 0)),
                    "neutral": float(sentiment.get("neutral", 0)),
                    "negative": float(sentiment.get("negative", 0))
                }
            
            # ç¡®ä¿risk_assessmentæ˜¯å­—å…¸å¹¶åŒ…å«å¿…è¦å­—æ®µ
            risk_assessment = report.get("risk_assessment", {})
            if isinstance(risk_assessment, dict):
                validated_report["risk_assessment"] = {
                    "level": risk_assessment.get("level", "medium"),
                    "factors": list(risk_assessment.get("factors", [])) if isinstance(risk_assessment.get("factors"), (list, tuple)) else [str(risk_assessment.get("factors", ""))]
                }
            
            # è®¾ç½®å…¶ä»–å­—æ®µ
            validated_report["trend_prediction"] = str(report.get("trend_prediction", "æš‚æ— è¶‹åŠ¿é¢„æµ‹"))
            
            # ç¡®ä¿recommendationsæ˜¯åˆ—è¡¨
            recommendations = report.get("recommendations", [])
            validated_report["recommendations"] = list(recommendations) if isinstance(recommendations, (list, tuple)) else [str(recommendations)]
            
            # ç¡®ä¿confidence_scoreæ˜¯0-1ä¹‹é—´çš„æµ®ç‚¹æ•°
            confidence = report.get("confidence_score", 0.5)
            try:
                confidence_score = float(confidence)
                validated_report["confidence_score"] = max(0.0, min(1.0, confidence_score))  # é™åˆ¶åœ¨0-1ä¹‹é—´
            except ValueError:
                validated_report["confidence_score"] = 0.5
            
            return validated_report
            
        except Exception as e:
            print(f"[VCS] æŠ¥å‘Šè§£æå¤±è´¥: {str(e)}")
            default_report["summary"] = f"æŠ¥å‘Šè§£æå¤±è´¥: {str(e)}"
            return default_report
    
    def _generate_error_report(self, risk_topic: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆé”™è¯¯æƒ…å†µä¸‹çš„æŠ¥å‘Š
        
        Args:
            risk_topic: å±é™©è¯é¢˜
            error_message: é”™è¯¯ä¿¡æ¯
            
        Returns:
            é”™è¯¯æŠ¥å‘Š
        """
        return {
            'summary': f"å¯¹è¯é¢˜ '{risk_topic.get('topic', 'æœªå‘½å')}' çš„åˆ†æå¤±è´¥",
            'key_findings': ['åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯'],
            'sentiment_analysis': {'positive': 0, 'neutral': 0, 'negative': 0},
            'risk_assessment': {'level': 'unknown', 'factors': [f'åˆ†æå¤±è´¥: {error_message}']},
            'trend_prediction': 'æ— æ³•é¢„æµ‹',
            'recommendations': ['è¯·æ£€æŸ¥çˆ¬è™«æ˜¯å¦æ­£å¸¸å·¥ä½œ', 'ç¡®è®¤çˆ¬å–ç»“æœæ ¼å¼æ˜¯å¦æ­£ç¡®', 'éªŒè¯LLMæœåŠ¡æ˜¯å¦å¯ç”¨'],
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'source_topic': risk_topic.get('topic', ''),
            'data_count': 0,
            'comment_count': 0,
            'confidence_score': 0.0
        }
    
    def _summarize_sub_reports(self, sub_reports: List[Dict[str, Any]], risk_topic: Dict[str, Any], important_videos: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ±‡æ€»æ‰€æœ‰å°æŠ¥å‘Šï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        
        Args:
            sub_reports: æ‰€æœ‰å…³é”®è¯çš„å°æŠ¥å‘Šåˆ—è¡¨
            risk_topic: åŸå§‹é£é™©è¯é¢˜
            important_videos: é‡è¦è§†é¢‘æ¥æºåˆ—è¡¨
            
        Returns:
            æœ€ç»ˆçš„ç»¼åˆåˆ†ææŠ¥å‘Š
        """
        logger.info(f"å¼€å§‹æ±‡æ€» {len(sub_reports)} ä¸ªå°æŠ¥å‘Š")
        
        # å¦‚æœæ²¡æœ‰å°æŠ¥å‘Šï¼Œè¿”å›é»˜è®¤æŠ¥å‘Š
        if not sub_reports:
            return {
                'summary': f"æœªç”Ÿæˆä»»ä½•å°æŠ¥å‘Šï¼Œæ— æ³•æ±‡æ€»åˆ†æ",
                'key_findings': [],
                'sentiment_analysis': {'positive': 0, 'neutral': 0, 'negative': 0},
                'risk_assessment': {'level': 'unknown', 'factors': ['æœªç”Ÿæˆä»»ä½•å°æŠ¥å‘Š']},
                'trend_prediction': 'æ— æ³•é¢„æµ‹',
                'recommendations': ['è¯·æ£€æŸ¥çˆ¬è™«æ˜¯å¦æ­£å¸¸å·¥ä½œ'],
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'source_topic': risk_topic.get('topic', ''),
                'data_count': 0,
                'comment_count': 0,
                'confidence_score': 0.0,
                'important_videos': important_videos
            }
        
        try:
            # å‡†å¤‡ç³»ç»Ÿæç¤ºè¯
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ†æƒ…åˆ†æä¸“å®¶ï¼Œæ“…é•¿ç»¼åˆå¤šä¸ªå°æŠ¥å‘Šç”Ÿæˆæœ€ç»ˆçš„åˆ†ææŠ¥å‘Šã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºJSONç»“æœï¼Œæé«˜é¢„è­¦é˜ˆå€¼ï¼Œåªå…³æ³¨çœŸæ­£å¯èƒ½å­˜åœ¨é£é™©çš„å†…å®¹ã€‚"
            
            # å‡†å¤‡ç”¨æˆ·æç¤ºè¯
            user_prompt = f"""
            è¯·æ ¹æ®ä»¥ä¸‹å¤šä¸ªå…³é”®è¯çš„å°æŠ¥å‘Šï¼Œç»¼åˆç”Ÿæˆä¸€ä»½æœ€ç»ˆçš„èˆ†æƒ…åˆ†ææŠ¥å‘Šã€‚
            
            ## é£é™©è¯é¢˜ä¿¡æ¯
            {json.dumps(risk_topic, ensure_ascii=False)}
            
            ## å°æŠ¥å‘Šåˆ—è¡¨
            {json.dumps(sub_reports, ensure_ascii=False)}
            
            ## é‡è¦è§†é¢‘æ¥æº
            {json.dumps(important_videos, ensure_ascii=False)}
            
            ## åˆ†æè¦æ±‚
            1. ç»¼åˆæ‰€æœ‰å°æŠ¥å‘Šçš„å…³é”®å‘ç°ï¼Œé¿å…é‡å¤
            2. å¯¹æ•´ä¸ªè¯é¢˜çš„é£é™©ç­‰çº§è¿›è¡Œç»¼åˆè¯„ä¼°
            3. åˆ†ææ•´ä½“æƒ…ç»ªå€¾å‘
            4. é¢„æµ‹è¯é¢˜çš„å‘å±•è¶‹åŠ¿
            5. æä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®
            6. å¯¹åˆ†æç»“æœçš„å¯ä¿¡åº¦è¿›è¡Œè¯„åˆ†ï¼ˆ0-1ï¼‰
            7. åœ¨æŠ¥å‘Šä¸­åŒ…å«é‡è¦è§†é¢‘æ¥æºä¿¡æ¯
            
            ## è¾“å‡ºæ ¼å¼
            è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - "summary": "æŠ¥å‘Šæ‘˜è¦"
            - "key_findings": ["å‘ç°1", "å‘ç°2", ...]
            - "sentiment_analysis": {{"positive": 0.0, "neutral": 0.0, "negative": 0.0}}
            - "risk_assessment": {{"level": "ä½/ä¸­/é«˜/æé«˜", "factors": ["å› ç´ 1", "å› ç´ 2", ...]}}
            - "trend_prediction": "è¶‹åŠ¿é¢„æµ‹"
            - "recommendations": ["å»ºè®®1", "å»ºè®®2", ...]
            - "confidence_score": 0.0-1.0
            - "important_videos": [{{"keyword": "å…³é”®è¯", "platform": "å¹³å°", "title": "æ ‡é¢˜", "url": "é“¾æ¥", "likes": æ•°å­—, "comments": æ•°å­—, "create_time": "æ—¶é—´"}}, ...]
            """
            
            # è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            response = self.llm.invoke(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                json_mode=True,
                temperature=0.1  # å¤§å¹…é™ä½æ¸©åº¦ï¼Œç¡®ä¿æŠ¥å‘Šä¸¥æ ¼åŸºäºäº‹å®ï¼Œå‡å°‘å¹»è§‰
            )
            
            final_report = json.loads(response)
            
            # éªŒè¯å¹¶è¡¥å……æŠ¥å‘Šå­—æ®µ
            if isinstance(final_report, dict):
                # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                if 'summary' not in final_report:
                    final_report['summary'] = "æœªç”Ÿæˆæ‘˜è¦"
                if 'key_findings' not in final_report:
                    final_report['key_findings'] = []
                if 'sentiment_analysis' not in final_report:
                    final_report['sentiment_analysis'] = {'positive': 0, 'neutral': 0, 'negative': 0}
                if 'risk_assessment' not in final_report:
                    final_report['risk_assessment'] = {'level': 'unknown', 'factors': []}
                if 'trend_prediction' not in final_report:
                    final_report['trend_prediction'] = "æ— æ³•é¢„æµ‹"
                if 'recommendations' not in final_report:
                    final_report['recommendations'] = []
                if 'confidence_score' not in final_report:
                    final_report['confidence_score'] = 0.5
                if 'important_videos' not in final_report:
                    final_report['important_videos'] = important_videos
                
                # æ·»åŠ å…ƒæ•°æ®
                final_report['timestamp'] = time.strftime("%Y-%m-%d %H:%M:%S")
                final_report['source_topic'] = risk_topic.get('topic', '')
                
                # è®¡ç®—æ€»æ•°æ®é‡
                total_data_count = sum(sub_report.get('data_count', 0) for sub_report in sub_reports)
                total_comment_count = sum(sub_report.get('comment_count', 0) for sub_report in sub_reports)
                final_report['data_count'] = total_data_count
                final_report['comment_count'] = total_comment_count
                
                return final_report
            else:
                # ä»sub_reportsä¸­æå–æ•°æ®
                total_data_count = sum(sub_report.get('data_count', 0) for sub_report in sub_reports)
                total_comment_count = sum(sub_report.get('comment_count', 0) for sub_report in sub_reports)
                
                # æå–æ‰€æœ‰å…³é”®å‘ç°
                all_key_findings = []
                for sub_report in sub_reports:
                    if 'key_findings' in sub_report and isinstance(sub_report['key_findings'], list):
                        all_key_findings.extend(sub_report['key_findings'])
                
                # æå–æ‰€æœ‰é£é™©å› ç´ 
                all_risk_factors = []
                for sub_report in sub_reports:
                    if 'risk_factors' in sub_report and isinstance(sub_report['risk_factors'], list):
                        all_risk_factors.extend(sub_report['risk_factors'])
                
                # è¿”å›åŒ…å«sub_reportsæ•°æ®çš„é»˜è®¤æŠ¥å‘Š
                return {
                    'summary': f"æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æŠ¥å‘Š",
                    'key_findings': all_key_findings if all_key_findings else [],
                    'sentiment_analysis': {'positive': 0, 'neutral': 0, 'negative': 0},
                    'risk_assessment': {'level': 'unknown', 'factors': all_risk_factors if all_risk_factors else ['æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥']},
                    'trend_prediction': 'æ— æ³•é¢„æµ‹',
                    'recommendations': ['è¯·æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦å¯ç”¨'],
                    'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                    'source_topic': risk_topic.get('topic', ''),
                    'data_count': total_data_count,
                    'comment_count': total_comment_count,
                    'confidence_score': 0.0,
                    'important_videos': important_videos
                }
        except Exception as e:
            logger.error(f"æ±‡æ€»å°æŠ¥å‘Šå¤±è´¥: {str(e)}", exc_info=True)
            # ä»sub_reportsä¸­æå–æ•°æ®
            total_data_count = sum(sub_report.get('data_count', 0) for sub_report in sub_reports)
            total_comment_count = sum(sub_report.get('comment_count', 0) for sub_report in sub_reports)
            
            # æå–æ‰€æœ‰å…³é”®å‘ç°
            all_key_findings = []
            for sub_report in sub_reports:
                if 'key_findings' in sub_report and isinstance(sub_report['key_findings'], list):
                    all_key_findings.extend(sub_report['key_findings'])
            
            # æå–æ‰€æœ‰é£é™©å› ç´ 
            all_risk_factors = []
            for sub_report in sub_reports:
                if 'risk_factors' in sub_report and isinstance(sub_report['risk_factors'], list):
                    all_risk_factors.extend(sub_report['risk_factors'])
            
            # è¿”å›åŒ…å«sub_reportsæ•°æ®çš„é»˜è®¤æŠ¥å‘Š
            return {
                'summary': f"æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼ŒåŸå› : {str(e)}",
                'key_findings': all_key_findings if all_key_findings else [],
                'sentiment_analysis': {'positive': 0, 'neutral': 0, 'negative': 0},
                'risk_assessment': {'level': 'unknown', 'factors': all_risk_factors if all_risk_factors else [f'æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}']},
                'trend_prediction': 'æ— æ³•é¢„æµ‹',
                'recommendations': ['è¯·æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦å¯ç”¨'],
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'source_topic': risk_topic.get('topic', ''),
                'data_count': total_data_count,
                'comment_count': total_comment_count,
                'confidence_score': 0.0,
                'important_videos': important_videos
            }
    
    def process_topic(self, risk_topic: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªå±é™©è¯é¢˜çš„å®Œæ•´æµç¨‹
        
        Args:
            risk_topic: æ¥è‡ªRisk Analyzerçš„å±é™©è¯é¢˜
            
        Returns:
            å®Œæ•´çš„åˆ†ææŠ¥å‘Š
        """
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        topic_name = risk_topic.get("topic", "æœªå‘½åè¯é¢˜")
        logger.info(f"å¼€å§‹å¤„ç†è¯é¢˜: {topic_name}")
        
        try:
            # 1. ç”Ÿæˆå…³é”®è¯å’Œçˆ¬å–é…ç½®
            logger.info("æ­¥éª¤1: ç”Ÿæˆå…³é”®è¯å’Œçˆ¬å–é…ç½®")
            crawl_config = self.generate_keywords(risk_topic)
            
            if not crawl_config or "keywords_config" not in crawl_config or len(crawl_config["keywords_config"]) == 0:
                raise ValueError("æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„å…³é”®è¯é…ç½®")
            
            # è®°å½•ç”Ÿæˆçš„å…³é”®è¯ï¼ˆé™åˆ¶æ˜¾ç¤ºæ•°é‡ï¼Œé¿å…è¿‡é•¿ï¼‰
            keywords_str = ', '.join([kw['keyword'] for kw in crawl_config['keywords_config'][:5]])
            if len(crawl_config['keywords_config']) > 5:
                keywords_str += '...'
            logger.info(f"æˆåŠŸç”Ÿæˆ {len(crawl_config['keywords_config'])} ä¸ªå…³é”®è¯: {keywords_str}")
            logger.info(f"ç›®æ ‡å¹³å°: {', '.join(crawl_config.get('platforms', []))}")
            
            # 2. è¾¹çˆ¬å–è¾¹åˆ†æï¼Œä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆå°æŠ¥å‘Š
            logger.info("æ­¥éª¤2: å¼€å§‹è¾¹çˆ¬å–è¾¹åˆ†æ")
            crawl_start_time = time.time()
            
            # å­˜å‚¨æ‰€æœ‰å¹³å°çš„çˆ¬å–ç»“æœ
            all_results = {
                'platform_results': {},
                'total_items': 0,
                'total_comments': 0
            }
            
            # å­˜å‚¨æ‰€æœ‰å°æŠ¥å‘Š
            sub_reports = []
            
            # å­˜å‚¨é‡è¦è§†é¢‘æ¥æº
            important_videos = []
            
            # éå†æ¯ä¸ªå…³é”®è¯ï¼Œä¸ºæ¯ä¸ªå…³é”®è¯å•ç‹¬æŒ‡å®šçˆ¬å–å‚æ•°
            for keyword_config in crawl_config['keywords_config']:
                keyword = keyword_config['keyword']
                max_video_count = keyword_config['max_video_count']
                max_comment_count = keyword_config['max_comment_count']
                max_retries = crawl_config.get('retries', 3)
                
                logger.info(f"å¤„ç†å…³é”®è¯: {keyword}ï¼Œè®¡åˆ’çˆ¬å– {max_video_count} ä¸ªè§†é¢‘/å¸–å­ï¼Œæ¯æ¡å†…å®¹ {max_comment_count} æ¡è¯„è®º")
                
                # ä¸ºå½“å‰å…³é”®è¯åˆ›å»ºçˆ¬å–ç»“æœå®¹å™¨
                keyword_results = {
                    'platform_results': {},
                    'total_items': 0,
                    'total_comments': 0
                }
                
                # éå†æ‰€æœ‰å¹³å°
                for platform in crawl_config['platforms']:
                    try:
                        logger.debug(f"åœ¨{platform}å¹³å°ä¸Šçˆ¬å–å…³é”®è¯: {keyword}")
                        # åˆå§‹åŒ–çˆ¬è™«
                        crawler = VideoCommentSpotter(platform=platform)
                        
                        # å³ä½¿æ²¡æœ‰å¯ç”¨çš„çˆ¬è™«ï¼Œä¹Ÿç»§ç»­æ‰§è¡Œï¼Œä»¥ä¾¿ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                        # VideoCommentSpotterç±»æ²¡æœ‰crawlerså±æ€§ï¼Œç›´æ¥è·³è¿‡æ£€æŸ¥
                        logger.debug(f"å¹³å° {platform} çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                        
                        # åˆ›å»ºçˆ¬å–é…ç½®ï¼Œä¼ é€’ç»™çˆ¬è™«
                        search_config = {
                            "crawler_max_notes_count": max_video_count,
                            "crawler_max_comments_count_single_notes": max_comment_count,
                            "enable_get_comments": True,
                            "max_retries": max_retries
                        }
                        
                        # çˆ¬å–å½“å‰å…³é”®è¯
                        logger.info(f"å¼€å§‹çˆ¬å–å…³é”®è¯: {keyword}ï¼Œå¹³å°: {platform}")
                        
                        platform_results = crawler.search(
                            keyword=keyword,
                            max_count=max_video_count,
                            max_retries=max_retries,
                            enable_get_comments=True,
                            platform_config=search_config
                        )
                        
                        # åˆå¹¶åˆ°å…³é”®è¯ç»“æœ
                        keyword_results['platform_results'][platform] = platform_results
                        platform_items = platform_results.get('total_items', 0)
                        platform_comments = platform_results.get('total_comments', 0)
                        keyword_results['total_items'] += platform_items
                        keyword_results['total_comments'] += platform_comments
                        
                        # åˆå¹¶åˆ°æ€»ç»“æœ
                        if platform not in all_results['platform_results']:
                            all_results['platform_results'][platform] = {
                                'results': [],
                                'total_items': 0,
                                'total_comments': 0,
                                'platform': platform
                            }
                        
                        if 'results' in platform_results:
                            all_results['platform_results'][platform]['results'].extend(platform_results['results'])
                        elif 'items' in platform_results:
                            # å¤„ç†å•ä¸ªå…³é”®è¯çˆ¬å–ç»“æœ
                            all_results['platform_results'][platform]['results'].append(platform_results)
                        
                        all_results['platform_results'][platform]['total_items'] += platform_items
                        all_results['platform_results'][platform]['total_comments'] += platform_comments
                        all_results['total_items'] += platform_items
                        all_results['total_comments'] += platform_comments
                        
                        # æ‰“å°çˆ¬å–å®Œæˆæ—¥å¿—
                        logger.info(f"âœ… {platform}å¹³å°å…³é”®è¯ {keyword} çˆ¬å–å®Œæˆ")
                        logger.info(f"ğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡:")
                        logger.info(f"   å®é™…è·å–å†…å®¹æ•°: {platform_items} æ¡")
                        logger.info(f"   å®é™…è·å–è¯„è®ºæ•°: {platform_comments} æ¡")
                        
                        logger.info(f"{platform}å¹³å°å…³é”®è¯ {keyword} çˆ¬å–å®Œæˆï¼Œè·å– {platform_items} æ¡å†…å®¹ï¼Œ{platform_comments} æ¡è¯„è®º")
                        
                        # æå–é‡è¦è§†é¢‘æ¥æºï¼ˆæ ¹æ®ç‚¹èµæ•°ã€è¯„è®ºæ•°ã€è§‚çœ‹é‡ç­‰ç»¼åˆåˆ¤æ–­ï¼‰
                        if 'items' in platform_results:
                            for item in platform_results['items']:
                                likes = item.get('likes', 0)
                                comments = len(item.get('comments', []))
                                views = item.get('views', 0)
                                
                                # æ›´ä¸¥æ ¼çš„è¿‡æ»¤æ¡ä»¶ï¼š
                                # 1. ç‚¹èµæ•° > 200 ä¸”è¯„è®ºæ•° > 20
                                # 2. æˆ–è€…è§‚çœ‹é‡ > 5000 ä¸”äº’åŠ¨ç‡ > 1%ï¼ˆäº’åŠ¨ç‡ = (ç‚¹èµ+è¯„è®º)/è§‚çœ‹é‡ï¼‰
                                engagement_rate = ((likes + comments) / views) * 100 if views > 0 else 0
                                
                                if (likes > 200 and comments > 20) or (views > 5000 and engagement_rate > 1):
                                    important_videos.append({
                                        'keyword': keyword,
                                        'platform': platform,
                                        'title': item.get('title', ''),
                                        'url': item.get('url', ''),
                                        'likes': likes,
                                        'comments': comments,
                                        'views': views,
                                        'create_time': item.get('create_time', '')
                                    })
                    except Exception as e:
                        logger.error(f"{platform}å¹³å°çˆ¬å–å…³é”®è¯ {keyword} å¤±è´¥: {str(e)}", exc_info=True)
                        keyword_results['platform_results'][platform] = {
                            'error': str(e),
                            'total_items': 0,
                            'total_comments': 0
                        }
                        
                        if platform not in all_results['platform_results']:
                            all_results['platform_results'][platform] = {
                                'results': [],
                                'total_items': 0,
                                'total_comments': 0,
                                'platform': platform
                            }
                    
                # ç«‹å³åˆ†æå½“å‰å…³é”®è¯çš„çˆ¬å–ç»“æœï¼Œç”Ÿæˆå°æŠ¥å‘Š
                logger.info(f"å¼€å§‹åˆ†æå…³é”®è¯ {keyword} çš„çˆ¬å–ç»“æœ")
                sub_report = self.analyze_content(keyword_results, risk_topic)
                sub_report['keyword'] = keyword
                sub_report['keyword_config'] = keyword_config
                sub_reports.append(sub_report)
                
                # æ‰“å°åˆ†æç»“æœåˆ°æ§åˆ¶å°ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹
                logger.info(f"\nå…³é”®è¯ {keyword} åˆ†æç»“æœ:")
                logger.info(f"é£é™©ç­‰çº§: {sub_report.get('risk_assessment', {}).get('level', 'unknown')}")
                logger.info(f"ç½®ä¿¡åº¦: {sub_report.get('confidence_score', 0.5):.2f}")
                logger.info(f"å…³é”®å‘ç°: {len(sub_report.get('key_findings', []))} é¡¹")
                if sub_report.get('key_findings'):
                    for i, finding in enumerate(sub_report['key_findings'][:3]):
                        logger.info(f"å‘ç° {i+1}: {finding[:50]}...")
                
                logger.info(f"å…³é”®è¯ {keyword} åˆ†æå®Œæˆï¼Œé£é™©ç­‰çº§: {sub_report.get('risk_assessment', {}).get('level', 'unknown')}")

            # è®¡ç®—çˆ¬å–æ—¶é—´
            crawl_time = round(time.time() - crawl_start_time, 2)
            logger.info(f"æ‰€æœ‰å¹³å°çˆ¬å–å®Œæˆï¼Œæ€»è€—æ—¶: {crawl_time} ç§’ï¼Œæ€»è®¡ {all_results['total_items']} æ¡å†…å®¹ï¼Œ{all_results['total_comments']} æ¡è¯„è®º")
            
            if all_results['total_items'] == 0:
                logger.warning("è­¦å‘Š: æœªçˆ¬å–åˆ°ä»»ä½•å†…å®¹")
            
            # å¤„ç†é‡è¦è§†é¢‘åˆ—è¡¨ï¼šå»é‡ã€æ’åºå’Œé™åˆ¶æ•°é‡
            if important_videos:
                logger.info(f"åŸå§‹é‡è¦è§†é¢‘æ•°é‡: {len(important_videos)}")
                
                # å»é‡ï¼šåŸºäºURLæˆ–æ ‡é¢˜
                unique_videos = {}
                for video in important_videos:
                    # ä½¿ç”¨è§†é¢‘URLæˆ–æ ‡é¢˜ä½œä¸ºå”¯ä¸€é”®
                    key = video.get('url', '') or video.get('title', '')
                    if key not in unique_videos:
                        unique_videos[key] = video
                
                # è½¬æ¢å›åˆ—è¡¨
                important_videos = list(unique_videos.values())
                logger.info(f"å»é‡åé‡è¦è§†é¢‘æ•°é‡: {len(important_videos)}")
                
                # æ’åºï¼šæŒ‰äº’åŠ¨é‡ï¼ˆç‚¹èµ+è¯„è®ºï¼‰é™åº
                important_videos.sort(key=lambda x: x.get('likes', 0) + x.get('comments', 0), reverse=True)
                
                # é™åˆ¶æ•°é‡ï¼Œæœ€å¤šæ˜¾ç¤º10ä¸ªé‡è¦è§†é¢‘
                important_videos = important_videos[:10]
                logger.info(f"æ’åºå’Œé™åˆ¶åé‡è¦è§†é¢‘æ•°é‡: {len(important_videos)}")
            
            # 3. æ±‡æ€»æ‰€æœ‰å°æŠ¥å‘Šï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            logger.info("æ­¥éª¤3: æ±‡æ€»æ‰€æœ‰å°æŠ¥å‘Šï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
            final_report = self._summarize_sub_reports(sub_reports, risk_topic, important_videos)
            
            analysis_time = round(time.time() - crawl_start_time, 2)
            total_time = round(time.time() - start_time, 2)
            logger.info(f"æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œé£é™©ç­‰çº§: {final_report.get('risk_assessment', {}).get('level', 'unknown')}")
            
            # æ‰“å°æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šåˆ°æ§åˆ¶å°ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹
            # è®°å½•æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šä¿¡æ¯åˆ°æ—¥å¿—
            logger.info(f"æœ€ç»ˆæ±‡æ€»æŠ¥å‘Š: è¯é¢˜-{topic_name}, å¤„ç†æ—¶é—´-{total_time}ç§’, é£é™©ç­‰çº§-{final_report.get('risk_assessment', {}).get('level', 'unknown')}")
            logger.info(f"æ€»çˆ¬å–å†…å®¹-{all_results['total_items']}æ¡, æ€»è¯„è®ºæ•°-{all_results['total_comments']}æ¡")
            
            # è®°å½•å¹³å°çˆ¬å–ç»Ÿè®¡åˆ°æ—¥å¿—
            for platform, result in all_results['platform_results'].items():
                logger.info(f"å¹³å°çˆ¬å–ç»Ÿè®¡: {platform}: {result['total_items']} æ¡å†…å®¹, {result['total_comments']} æ¡è¯„è®º")
            
            # è®°å½•é£é™©è¯„ä¼°ç»“æœåˆ°æ—¥å¿—
            risk_factors = final_report.get('risk_assessment', {}).get('factors', [])
            logger.info(f"é£é™©è¯„ä¼°ç»“æœ: é£é™©ç­‰çº§-{final_report.get('risk_assessment', {}).get('level', 'unknown')}, ç½®ä¿¡åº¦-{final_report.get('confidence_score', 0.5):.2f}")
            if risk_factors:
                logger.info(f"é£é™©å› ç´ : {len(risk_factors)} é¡¹")
                for i, factor in enumerate(risk_factors):
                    logger.info(f"é£é™©å› ç´  {i+1}: {factor}")
            
            # è®°å½•æƒ…æ„Ÿåˆ†æç»“æœåˆ°æ—¥å¿—
            sentiment = final_report.get('sentiment_analysis', {})
            emotion_score = sentiment.get('negative', 0) * 100
            logger.info(f"æƒ…æ„Ÿåˆ†æ: æƒ…æ„Ÿæ‰“åˆ†-{emotion_score:.0f}/100")
            
            # è®°å½•å…³é”®å‘ç°åˆ°æ—¥å¿—
            key_findings = final_report.get('key_findings', [])
            logger.info(f"å…³é”®å‘ç°: {len(key_findings)} é¡¹")
            for i, finding in enumerate(key_findings[:3]):
                logger.info(f"å…³é”®å‘ç° {i+1}: {finding[:50]}...")
            
            # è®°å½•è¶‹åŠ¿é¢„æµ‹å’Œå»ºè®®åˆ°æ—¥å¿—
            logger.info(f"è¶‹åŠ¿é¢„æµ‹: {final_report.get('trend_prediction', 'æ— æ³•é¢„æµ‹')}")
            recommendations = final_report.get('recommendations', [])
            logger.info(f"å»ºè®®: {len(recommendations)} æ¡")
            
            # è®°å½•é‡è¦è§†é¢‘æ¥æºåˆ°æ—¥å¿—
            logger.info(f"é‡è¦è§†é¢‘æ¥æº: {len(important_videos)} ä¸ª")
            for i, video in enumerate(important_videos[:3]):  # åªè®°å½•å‰3ä¸ª
                title = video.get('title', '')[:40]
                logger.info(f"é‡è¦è§†é¢‘ {i+1}: {title}..., å¹³å°-{video.get('platform', '')}")
            
            # 4. æ›´æ–°è®°å¿†
            logger.info("æ­¥éª¤4: æ›´æ–°è®°å¿†")
            memory_item = {
                'timestamp': time.time(),
                'topic': topic_name,
                'keywords': [kw['keyword'] for kw in crawl_config['keywords_config']],
                'total_items': all_results['total_items'],
                'risk_level': final_report.get('risk_assessment', {}).get('level', 'medium')
            }
            self.memory.append(memory_item)
            try:
                self._save_memory()
                logger.debug("è®°å¿†ä¿å­˜æˆåŠŸ")
            except Exception as e:
                logger.error(f"è®°å¿†ä¿å­˜å¤±è´¥: {str(e)}", exc_info=True)
            
            # 5. ä¿å­˜å®Œæ•´æŠ¥å‘Š
            logger.info("æ­¥éª¤5: ä¿å­˜åˆ†ææŠ¥å‘Š")
            full_report = {
                'risk_topic': risk_topic,
                'crawl_config': crawl_config,
                'crawl_results': all_results,
                'sub_reports': sub_reports,
                'analysis': final_report,
                'important_videos': important_videos
            }
            
            # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
            report_filename = f"vcs_report_{time.strftime('%Y%m%d_%H%M%S')}.json"
            report_path = self.output_dir / report_filename
            try:
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(full_report, f, ensure_ascii=False, indent=2)
                logger.info(f"åˆ†ææŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°: {report_path}")
            except Exception as e:
                logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}", exc_info=True)
            
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = round(time.time() - start_time, 2)
            logger.info(f"è¯é¢˜ '{topic_name}' å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶ {execution_time} ç§’")
            
            # 6. è¿”å›ç»™Risk Analyzerçš„è¯¦å°½æŠ¥å‘Š
            return {
                'status': 'success',
                'source_topic': risk_topic,
                'summary': final_report['summary'],
                'key_findings': final_report['key_findings'],
                'risk_assessment': final_report['risk_assessment'],
                'analysis': final_report,  # åŒ…å«å®Œæ•´çš„åˆ†ææŠ¥å‘Š
                'data_statistics': {
                    'total_platforms': len(crawl_config['platforms']),
                    'total_keywords': len(crawl_config['keywords_config']),
                    'total_items': all_results['total_items'],
                    'total_comments': all_results['total_comments']
                },
                'report_path': str(report_path),
                'execution_time': execution_time,
                'timestamp': final_report['timestamp'],
                'important_videos': important_videos
            }
            
        except KeyError as ke:
            error_msg = f"ç¼ºå°‘å¿…è¦å­—æ®µ: {str(ke)}"
            logger.error(f"å¤„ç†è¯é¢˜ '{topic_name}' æ—¶å‡ºé”™: {error_msg}", exc_info=True)
        except ValueError as ve:
            error_msg = str(ve)
            logger.error(f"å¤„ç†è¯é¢˜ '{topic_name}' æ—¶å‡ºç°å€¼é”™è¯¯: {error_msg}", exc_info=True)
        except Exception as e:
            error_msg = str(e)
            logger.error(f"å¤„ç†è¯é¢˜ '{topic_name}' æ—¶å‡ºé”™: {error_msg}", exc_info=True)
        
        # ç”Ÿæˆé”™è¯¯æŠ¥å‘Š
        error_report = self._generate_error_report(risk_topic, error_msg)
        
        # ä¿å­˜é”™è¯¯æŠ¥å‘Š
        full_error_report = {
            'risk_topic': risk_topic,
            'error': error_msg,
            'analysis': error_report
        }
        error_report_filename = f"vcs_error_report_{time.strftime('%Y%m%d_%H%M%S')}.json"
        error_report_path = self.output_dir / error_report_filename
        
        try:
            with open(error_report_path, 'w', encoding='utf-8') as f:
                json.dump(full_error_report, f, ensure_ascii=False, indent=2)
            logger.info(f"é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {error_report_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜é”™è¯¯æŠ¥å‘Šå¤±è´¥: {str(e)}", exc_info=True)
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = round(time.time() - start_time, 2)
        logger.info(f"è¯é¢˜ '{topic_name}' å¤„ç†å¤±è´¥ï¼Œæ€»è€—æ—¶ {execution_time} ç§’")
        
        return {
            'status': 'error',
            'error': error_msg,
            'error_report': error_report,
            'report_path': str(error_report_path),
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }

    def handle_risk_analyzer_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†æ¥è‡ªRisk Analyzerçš„è¯·æ±‚ï¼Œæ ‡å‡†åŒ–æ¥å£
        
        Args:
            request_data: æ¥è‡ªRisk Analyzerçš„æ•°æ®ï¼Œå¿…é¡»åŒ…å«topicå­—æ®µ
            
        Returns:
            æ ‡å‡†åŒ–çš„å“åº”æ•°æ®ï¼ŒåŒ…å«åˆ†æç»“æœå’Œå…ƒæ•°æ®
        """
        request_id = request_data.get("request_id", str(int(time.time())))
        logger.info(f"æ”¶åˆ°æ¥è‡ªRisk Analyzerçš„è¯·æ±‚ï¼Œè¯·æ±‚ID: {request_id}")
        
        try:
            # ä½¿ç”¨ä¸“ç”¨æ–¹æ³•éªŒè¯è¯·æ±‚æ ¼å¼
            is_valid, error_msg = self.validate_risk_topic_format(request_data)
            if not is_valid:
                logger.warning(f"è¯·æ±‚éªŒè¯å¤±è´¥ï¼Œè¯·æ±‚ID: {request_id}ï¼Œé”™è¯¯: {error_msg}")
                raise ValueError(error_msg)
            
            # è®°å½•è¯·æ±‚ä¿¡æ¯ï¼ˆæ•æ„Ÿä¿¡æ¯è„±æ•ï¼‰
            topic_name = request_data.get("topic", "æœªå‘½å")
            priority = request_data.get("priority", "medium")
            logger.info(f"å¤„ç†é£é™©è¯é¢˜: {topic_name} (ä¼˜å…ˆçº§: {priority})ï¼Œè¯·æ±‚ID: {request_id}")
            logger.debug(f"å®Œæ•´è¯·æ±‚æ•°æ®: {json.dumps(request_data, ensure_ascii=False, default=str)}")
            
            # æ‰§è¡Œå®Œæ•´çš„è¯é¢˜å¤„ç†æµç¨‹
            start_time = time.time()
            process_result = self.process_topic(request_data)
            process_time = time.time() - start_time
            
            # æ ‡å‡†åŒ–å“åº”æ ¼å¼
            response = {}
            if process_result.get("status") == "success":
                # æˆåŠŸæƒ…å†µçš„æ ‡å‡†åŒ–å“åº” - è¿”å›è¯¦å°½æŠ¥å‘Š
                response = {
                    "status": "success",
                    "data": {
                        "risk_topic": request_data,
                        "analysis_summary": process_result.get("summary", ""),
                        "key_findings": process_result.get("key_findings", []),
                        "risk_assessment": process_result.get("risk_assessment", {}),
                        "sentiment_analysis": process_result.get("analysis", {}).get("sentiment_analysis", {}),
                        "trend_prediction": process_result.get("analysis", {}).get("trend_prediction", ""),
                        "recommendations": process_result.get("analysis", {}).get("recommendations", []),
                        "confidence_score": process_result.get("analysis", {}).get("confidence_score", 0.5),
                        "important_videos": process_result.get("important_videos", []),
                        "detailed_report": process_result.get("analysis", {})
                    },
                    "metadata": {
                        "processing_time": round(process_time, 2),
                        "data_statistics": process_result.get("data_statistics", {}),
                        "report_path": process_result.get("report_path", ""),
                        "timestamp": process_result.get("timestamp", time.strftime("%Y-%m-%d %H:%M:%S")),
                        "request_id": request_id,
                        "total_items": process_result.get("data_statistics", {}).get("total_items", 0),
                        "total_comments": process_result.get("data_statistics", {}).get("total_comments", 0),
                        "total_keywords": process_result.get("data_statistics", {}).get("total_keywords", 0),
                        "total_platforms": process_result.get("data_statistics", {}).get("total_platforms", 0)
                    },
                    "error": None
                }
                risk_level = process_result.get("risk_assessment", {}).get("level", "medium")
                logger.info(f"è¯é¢˜ '{topic_name}' åˆ†ææˆåŠŸï¼Œé£é™©ç­‰çº§: {risk_level}ï¼Œè¯·æ±‚ID: {request_id}")
            else:
                # å¤±è´¥æƒ…å†µçš„æ ‡å‡†åŒ–å“åº”
                response = {
                    "status": "error",
                    "data": None,
                    "metadata": {
                        "timestamp": process_result.get("timestamp", time.strftime("%Y-%m-%d %H:%M:%S")),
                        "report_path": process_result.get("report_path", ""),
                        "processing_time": round(process_time, 2),
                        "request_id": request_id
                    },
                    "error": {
                        "message": process_result.get("error", "å¤„ç†å¤±è´¥"),
                        "code": "PROCESSING_FAILED"
                    }
                }
                logger.error(f"è¯é¢˜ '{topic_name}' åˆ†æå¤±è´¥ï¼Œé”™è¯¯: {response['error']['message']}ï¼Œè¯·æ±‚ID: {request_id}")
            
            logger.debug(f"å“åº”æ•°æ®: {json.dumps(response, ensure_ascii=False, default=str)}")
            return response
                
        except ValueError as ve:
            error_msg = str(ve)
            logger.warning(f"è¯·æ±‚éªŒè¯å¤±è´¥ï¼Œè¯·æ±‚ID: {request_id}ï¼Œé”™è¯¯: {error_msg}")
            return {
                "status": "error",
                "data": None,
                "metadata": {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "request_id": request_id
                },
                "error": {
                    "message": error_msg,
                    "code": "VALIDATION_ERROR"
                }
            }
        except KeyError as ke:
            error_msg = f"ç¼ºå°‘å¿…è¦å­—æ®µ: {str(ke)}"
            logger.error(f"å¤„ç†è¯·æ±‚æ—¶ç¼ºå°‘å¿…è¦å­—æ®µï¼Œè¯·æ±‚ID: {request_id}ï¼Œé”™è¯¯: {error_msg}", exc_info=True)
            return {
                "status": "error",
                "data": None,
                "metadata": {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "request_id": request_id
                },
                "error": {
                    "message": error_msg,
                    "code": "MISSING_FIELD"
                }
            }
        except Exception as e:
            error_msg = str(e)
            logger.error(f"å¤„ç†Risk Analyzerè¯·æ±‚æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯ï¼Œè¯·æ±‚ID: {request_id}ï¼Œé”™è¯¯: {error_msg}", exc_info=True)
            return {
                "status": "error",
                "data": None,
                "metadata": {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "request_id": request_id
                },
                "error": {
                    "message": "å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯",
                    "code": "INTERNAL_ERROR"
                }
            }
    
    async def handle_risk_analyzer_request_async(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¼‚æ­¥å¤„ç†æ¥è‡ªRisk Analyzerçš„è¯·æ±‚
        
        Args:
            request_data: åŒ…å«é£é™©è¯é¢˜ä¿¡æ¯çš„è¯·æ±‚æ•°æ®
            
        Returns:
            æ ‡å‡†åŒ–çš„å“åº”æ•°æ®
        """
        request_id = request_data.get("request_id", str(int(time.time())))
        topic_name = request_data.get("topic", "æœªå‘½å")
        logger.info(f"å¼€å§‹å¼‚æ­¥å¤„ç†æ¥è‡ªRisk Analyzerçš„è¯·æ±‚ï¼Œè¯·æ±‚ID: {request_id}ï¼Œè¯é¢˜: {topic_name}")
        
        try:
            # åœ¨äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡ŒåŒæ­¥å¤„ç†é€»è¾‘
            logger.debug(f"å°†è¯·æ±‚æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œï¼Œè¯·æ±‚ID: {request_id}")
            return await asyncio.get_event_loop().run_in_executor(
                None, 
                self.handle_risk_analyzer_request, 
                request_data
            )
        except asyncio.TimeoutError:
            error_msg = "è¯·æ±‚å¤„ç†è¶…æ—¶"
            logger.error(f"å¼‚æ­¥å¤„ç†è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ±‚ID: {request_id}")
            return {
                "status": "error",
                "data": None,
                "metadata": {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "request_id": request_id
                },
                "error": {
                    "message": error_msg,
                    "code": "TIMEOUT_ERROR"
                }
            }
        except Exception as e:
            error_msg = str(e)
            logger.error(f"å¼‚æ­¥å¤„ç†è¯·æ±‚æ—¶å‡ºé”™ï¼Œè¯·æ±‚ID: {request_id}ï¼Œé”™è¯¯: {error_msg}", exc_info=True)
            return {
                "status": "error",
                "data": None,
                "metadata": {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "request_id": request_id
                },
                "error": {
                    "message": "å¼‚æ­¥å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯",
                    "code": "ASYNC_PROCESSING_ERROR"
                }
            }
    
    def analyze_topic(self, keywords_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æè¯é¢˜ï¼Œç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
        
        Args:
            keywords_result: åŒ…å«å…³é”®è¯çš„å­—å…¸
            
        Returns:
            è¯¦ç»†çš„åˆ†ææŠ¥å‘Š
        """
        # ä¸ºäº†ä¿æŒä¸main.pyä¸­è°ƒç”¨çš„å…¼å®¹æ€§ï¼Œè¿™é‡Œå®ç°analyze_topicæ–¹æ³•
        # å®é™…ä¸Šåº”è¯¥è°ƒç”¨process_topicæ–¹æ³•è¿›è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹
        topic = keywords_result.get('topic', 'æœªå‘½åè¯é¢˜')
        risk_topic = {
            'topic': topic,
            'platform': 'ç¤¾äº¤åª’ä½“',
            'hotness': 'é«˜',
            'risk_level': 4,
            'category': 'äº§å“è´¨é‡',
            'reason': 'å¤šä¸ªç”¨æˆ·åæ˜ ç›¸å…³é—®é¢˜',
            'further_investigate': True
        }
        
        print(f"\n[VideosCommentsSpotter] å¼€å§‹åˆ†æè¯é¢˜: {topic}")
        print(f"[VideosCommentsSpotter] ä½¿ç”¨å…³é”®è¯: {', '.join(keywords_result.get('keywords', []))}")
        
        # è°ƒç”¨process_topicè¿›è¡Œå®Œæ•´å¤„ç†
        result = self.process_topic(risk_topic)
        
        # æ‰“å°åˆ†æç»“æœ
        print(f"\n[VideosCommentsSpotter] è¯é¢˜åˆ†æå®Œæˆ")
        print(f"[VideosCommentsSpotter] åˆ†æçŠ¶æ€: {result.get('status', 'æœªçŸ¥')}")
        
        if result.get('status') == 'success':
            print(f"[VideosCommentsSpotter] é£é™©è¯„ä¼°: {result.get('risk_assessment', {}).get('level', 'æœªçŸ¥')}")
            print(f"[VideosCommentsSpotter] ç½®ä¿¡åº¦: {result.get('confidence_score', 0.5):.2f}")
            print(f"[VideosCommentsSpotter] å…³é”®å‘ç°: {len(result.get('key_findings', []))} é¡¹")
        else:
            print(f"[VideosCommentsSpotter] åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        return result
    
    def validate_risk_topic_format(self, request_data: Dict[str, Any]) -> tuple[bool, str]:
        """
        éªŒè¯é£é™©è¯é¢˜è¯·æ±‚çš„æ ¼å¼
        
        Args:
            request_data: è¯·æ±‚æ•°æ®
            
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        logger.debug("å¼€å§‹éªŒè¯è¯·æ±‚æ ¼å¼")
        
        # åŸºç¡€ç±»å‹éªŒè¯
        if not isinstance(request_data, dict):
            logger.warning("è¯·æ±‚éªŒè¯å¤±è´¥: æ•°æ®ç±»å‹ä¸æ˜¯å­—å…¸")
            return False, "è¯·æ±‚æ•°æ®å¿…é¡»æ˜¯å­—å…¸æ ¼å¼"
        
        # å¿…éœ€å­—æ®µéªŒè¯
        required_fields = ["topic"]
        for field in required_fields:
            if field not in request_data:
                logger.warning(f"è¯·æ±‚éªŒè¯å¤±è´¥: ç¼ºå°‘å¿…éœ€å­—æ®µ '{field}'")
                return False, f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}"
            if not request_data[field]:
                logger.warning(f"è¯·æ±‚éªŒè¯å¤±è´¥: å­—æ®µ '{field}' çš„å€¼ä¸ºç©º")
                return False, f"å­—æ®µ '{field}' çš„å€¼ä¸èƒ½ä¸ºç©º"
        
        # topicå­—æ®µé•¿åº¦éªŒè¯
        topic = request_data["topic"]
        if not isinstance(topic, str) or len(topic) > 500:
            logger.warning(f"è¯·æ±‚éªŒè¯å¤±è´¥: topicå­—æ®µé•¿åº¦æ— æ•ˆ (é•¿åº¦: {len(topic) if isinstance(topic, str) else 'æ— æ•ˆç±»å‹'})")
            return False, "topicå­—æ®µé•¿åº¦å¿…é¡»åœ¨1-500ä¸ªå­—ç¬¦ä¹‹é—´"
            
        # å¯é€‰å­—æ®µéªŒè¯
        if "priority" in request_data:
            if request_data["priority"] not in ["low", "medium", "high"]:
                logger.warning(f"è¯·æ±‚éªŒè¯å¤±è´¥: priorityå­—æ®µå€¼æ— æ•ˆ (å€¼: {request_data['priority']})")
                return False, "priorityå­—æ®µå€¼å¿…é¡»æ˜¯: low, medium æˆ– high"
        
        # å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯éªŒè¯
        if "context" in request_data and not isinstance(request_data["context"], dict):
            logger.warning("è¯·æ±‚éªŒè¯å¤±è´¥: contextå­—æ®µå¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            return False, "contextå­—æ®µå¿…é¡»æ˜¯å­—å…¸æ ¼å¼"
        
        logger.debug("è¯·æ±‚æ ¼å¼éªŒè¯é€šè¿‡")
        return True, ""


def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    llm_client = LLMClient(config=LLM_CONFIG)
    
    # åˆå§‹åŒ–Agent
    agent = VideosCommentsSpotterAgent(llm_client=llm_client)
    
    # æµ‹è¯•ç¤ºä¾‹è¯é¢˜
    test_topic = {
        "topic": "äº§å“å®‰å…¨éšæ‚£",
        "platform": "ç¤¾äº¤åª’ä½“",
        "hotness": "é«˜",
        "risk_level": 4,
        "category": "äº§å“è´¨é‡",
        "reason": "å¤šä¸ªç”¨æˆ·åæ˜ äº§å“å­˜åœ¨å®‰å…¨éšæ‚£",
        "further_investigate": True
    }
    
    # ä½¿ç”¨æ¥å£å‡½æ•°å¤„ç†è¯é¢˜
    print("[VCS] æµ‹è¯•æ¥å£è°ƒç”¨...")
    result = agent.handle_risk_analyzer_request(test_topic)
    print("[VCS] æ¥å£å“åº”:")
    print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()