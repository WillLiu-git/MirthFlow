# HotspotHunter/agent.py

import time
import json
import logging
from pathlib import Path
from typing import List, Dict, Any
import sys
import os

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger('HotspotHunter')
logger.setLevel(logging.INFO)

# --- å¯¼å…¥ä¾èµ– (åŒ…å«å›é€€æœºåˆ¶ï¼Œç”¨äºå…¼å®¹ä¸åŒè¿è¡Œç¯å¢ƒ) ---
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆä½œä¸ºåŒ…çš„ä¸€éƒ¨åˆ†æ—¶ï¼‰
    from .llm import LLMClient
    from .utils.config import TOPHUB_URLS, OUTPUT_DIRECTORY, LLM_CONFIG, HOTSPOT_HUNTER_INTERVAL
    from .tools.hotlist_crawler import hotlist_crawler
    from .prompts.prompts import HH_SCAN_PROMPT
except ImportError:
    # å›é€€åˆ°ç»å¯¹å¯¼å…¥ï¼ˆç›´æ¥è¿è¡Œæ—¶ï¼‰
    from llm import LLMClient
    from utils.config import TOPHUB_URLS, OUTPUT_DIRECTORY, LLM_CONFIG, HOTSPOT_HUNTER_INTERVAL
    from tools.hotlist_crawler import hotlist_crawler
    from prompts.prompts import HH_SCAN_PROMPT


# å®šä¹‰æƒ…æŠ¥ç«™æ–‡ä»¶è·¯å¾„ (æ ¸å¿ƒäº¤äº’æ–‡ä»¶)
INTELLIGENCE_FILE = Path(OUTPUT_DIRECTORY) / "intelligence_feed.json"
INTELLIGENCE_FILE.parent.mkdir(parents=True, exist_ok=True)


class HotspotHunterAgent:
    """
    Hotspot Hunter Agent (ä¾¦å¯Ÿå…µ/ç”Ÿäº§è€…):
    1. å®šæœŸçˆ¬å–æ¦œå•ï¼Œä½¿ç”¨ LLM è¿›è¡Œç»“æ„åŒ–é£é™©åˆç­›ã€‚
    2. å°†ç»“æ„åŒ–é£é™©è¯é¢˜å†™å…¥æƒ…æŠ¥ç«™ (intelligence_feed.json)ã€‚
    """

    def __init__(self, llm_client: LLMClient, crawl_interval: int = 30):
        self.llm = llm_client
        self.interval = crawl_interval  # å¯è¢« Risk Analyzer åŠ¨æ€ä¿®æ”¹
        self.memory_file = Path(OUTPUT_DIRECTORY) / "hh_memory.json"
        self.memory: List[Dict] = self._load_memory()

    def _load_memory(self) -> List[Dict]:
        if self.memory_file.exists():
            with open(self.memory_file, "r", encoding="utf-8") as f:
                try:
                    # åªåŠ è½½æœ€è¿‘ 200 æ¡è®°å¿†
                    return json.load(f)[-200:]
                except json.JSONDecodeError:
                    return []
        return []

    def _save_memory(self):
        self.memory_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.memory_file, "w", encoding="utf-8") as f:
            # åªä¿å­˜æœ€è¿‘ 1000 æ¡è®°å¿†
            json.dump(self.memory[-1000:], f, ensure_ascii=False, indent=2)

    def _append_to_intelligence(self, risk_topics: List[Dict[str, Any]]):
        """å°† LLM ç­›é€‰å‡ºçš„é£é™©è¯é¢˜è¿½åŠ åˆ°æƒ…æŠ¥ç«™æ–‡ä»¶ã€‚"""
        if not risk_topics:
            return

        # ç¡®ä¿risk_topicsæ˜¯ä¸€ä¸ªåˆ—è¡¨
        if not isinstance(risk_topics, list):
            risk_topics = [risk_topics]

        current_data = []
        if INTELLIGENCE_FILE.exists():
            with open(INTELLIGENCE_FILE, "r", encoding="utf-8") as f:
                try:
                    current_data = json.load(f)
                    # ç¡®ä¿current_dataæ˜¯ä¸€ä¸ªåˆ—è¡¨
                    if not isinstance(current_data, list):
                        current_data = []
                except json.JSONDecodeError:
                    current_data = []

        # è¿‡æ»¤æ‰éå­—å…¸ç±»å‹çš„æ•°æ®
        valid_topics = [topic for topic in risk_topics if isinstance(topic, dict)]
        current_data.extend(valid_topics)

        # é™åˆ¶æ–‡ä»¶å¤§å°ï¼Œåªä¿ç•™æœ€è¿‘100æ¡è®°å½•
        if len(current_data) > 100:
            current_data = current_data[-100:]

        with open(INTELLIGENCE_FILE, "w", encoding="utf-8") as f:
            json.dump(current_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æˆåŠŸå†™å…¥ {len(valid_topics)} æ¡æœ‰æ•ˆé£é™©æ•°æ®åˆ°æƒ…æŠ¥ç«™")

    def _analyze_hotspot(self, scraped_data_json: str) -> Dict[str, Any]:
        """è°ƒç”¨ LLM å¯¹çˆ¬å–æ•°æ®è¿›è¡Œåˆ†æï¼Œè¿”å›å®Œæ•´çš„ç»“æ„åŒ–é£é™©æŠ¥å‘Šã€‚"""
        # å¢å¼ºçš„ç³»ç»Ÿæç¤ºï¼Œæ˜ç¡®è¦æ±‚ä¸¥æ ¼çš„JSONæ ¼å¼å’ŒåŸºäºäº‹å®çš„åˆ†æ
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªèˆ†æƒ…é£é™©ä¾¦å¯Ÿå…µï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¾“å‡ºç»“æœã€‚

ã€æ ¸å¿ƒåŸåˆ™ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘ï¼š
1. **ä¸¥æ ¼åŸºäºäº‹å®**ï¼šåªåˆ†æè¾“å…¥æ•°æ®ä¸­æ˜ç¡®æåŠçš„å†…å®¹ï¼Œä¸å¾—æ·»åŠ ä»»ä½•æœªæåŠçš„ä¿¡æ¯
2. **é¿å…å¹»è§‰**ï¼šä¸å¾—æ¨æµ‹ã€æƒ³è±¡æˆ–æ·»åŠ ä»»ä½•æœªåœ¨è¾“å…¥æ•°æ®ä¸­å‡ºç°çš„ä¿¡æ¯
3. **æé«˜é£é™©é˜ˆå€¼**ï¼šåªå°†çœŸæ­£å­˜åœ¨è´Ÿé¢èˆ†æƒ…é£é™©çš„è¯é¢˜æ ‡è®°ä¸ºé£é™©ï¼Œé¿å…è¿‡åº¦æ•æ„Ÿ
4. **è¯æ®å¯¼å‘**ï¼šæ¯ä¸ªé£é™©åˆ¤æ–­å¿…é¡»æœ‰æ˜ç¡®çš„è¯æ®æ”¯æŒï¼Œä¸èƒ½ä»…å‡­ä¸»è§‚åˆ¤æ–­
5. **ç²¾å‡†è¯†åˆ«**ï¼šåªè¯†åˆ«é‚£äº›å…·æœ‰æ˜ç¡®è´Ÿé¢å€¾å‘ã€äº‰è®®æ€§æˆ–æ½œåœ¨å±å®³çš„è¯é¢˜

ã€è¾“å‡ºè¦æ±‚ã€‘ï¼š
- ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„JSONå¯¹è±¡ï¼ŒåŒ…å«å®Œæ•´çš„'summary'å’Œ'items'å­—æ®µ
- ä¸åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€æ ‡è®°æˆ–è§£é‡Š
- å¦‚æœè¾“å…¥æ•°æ®ä¸­æ²¡æœ‰æ˜æ˜¾çš„é£é™©è¯é¢˜ï¼Œitems å¿…é¡»ä¸ºç©ºæ•°ç»„
- æ¯ä¸ªé£é™©é¡¹ç›®çš„ reason å¿…é¡»åŸºäºè¾“å…¥æ•°æ®ä¸­çš„å…·ä½“å†…å®¹ï¼Œä¸å¾—æ·»åŠ æœªæåŠçš„ä¿¡æ¯"""

        # ä¼ å…¥å†å²è®°å¿†å¸®åŠ©LLMè¿›è¡Œå»é‡å’Œè¶‹åŠ¿åˆ¤æ–­
        historical_data_json = json.dumps(self.memory[-10:], ensure_ascii=False)

        # ä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥è€Œä¸æ˜¯format()æ–¹æ³•ï¼Œé¿å…è§£æJSONæ¨¡æ¿ä¸­çš„{}å­—ç¬¦
        user_prompt = HH_SCAN_PROMPT
        user_prompt = user_prompt.replace('{crawled_data}', scraped_data_json)
        user_prompt = user_prompt.replace('{historical_data}', historical_data_json)

        try:
            risk_json_str = self.llm.invoke(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                json_mode=True,
                temperature=0.1  # å¤§å¹…é™ä½æ¸©åº¦ï¼Œç¡®ä¿åˆ†æä¸¥æ ¼åŸºäºäº‹å®ï¼Œå‡å°‘å¹»è§‰
            )
            
            # å°è¯•ç›´æ¥è§£æ
            parsed_result = json.loads(risk_json_str)
            
            # éªŒè¯ç»“æœç»“æ„æ˜¯å¦å®Œæ•´
            if isinstance(parsed_result, dict):
                # ç¡®ä¿è¿”å›çš„ç»“æœåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
                if 'summary' not in parsed_result:
                    parsed_result['summary'] = "æœªç”Ÿæˆæ‘˜è¦"
                if 'items' not in parsed_result:
                    parsed_result['items'] = []
                
                # éªŒè¯å’Œæ¸…ç†æ¯ä¸ªé£é™©é¡¹ç›®ï¼Œç¡®ä¿ä¸¥æ ¼åŸºäºäº‹å®
                valid_items = []
                for item in parsed_result['items']:
                    # éªŒè¯å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨ä¸”æœ‰å®é™…å†…å®¹
                    topic = item.get('topic') or item.get('title', '')
                    reason = item.get('reason', '').strip()
                    
                    # å¦‚æœ topic ä¸ºç©ºæˆ– reason ä¸ºç©ºï¼Œè·³è¿‡è¯¥é¡¹ç›®ï¼ˆé¿å…å¹»è§‰ï¼‰
                    if not topic or not reason:
                        logger.warning(f"è·³è¿‡æ— æ•ˆçš„é£é™©é¡¹ç›®: topic={topic}, reason={reason}")
                        continue
                    
                    # éªŒè¯ reason æ˜¯å¦åŒ…å«å…·ä½“è¯æ®ï¼ˆè‡³å°‘10ä¸ªå­—ç¬¦ï¼Œé¿å…ç©ºæ³›åˆ¤æ–­ï¼‰
                    if len(reason) < 10:
                        logger.warning(f"è·³è¿‡reasonè¿‡çŸ­çš„é£é™©é¡¹ç›®: {topic}, reasoné•¿åº¦={len(reason)}")
                        continue
                    
                    # ç¡®ä¿æ¯ä¸ªé¡¹ç›®éƒ½æœ‰å…·ä½“çš„é£é™©ç­‰çº§
                    risk_level = item.get('risk_level')
                    if risk_level is None or risk_level == 'æœªçŸ¥' or (isinstance(risk_level, str) and risk_level.strip() == ''):
                        # å¦‚æœé£é™©ç­‰çº§æ— æ•ˆï¼Œæ ¹æ®reasonä¸­çš„è´Ÿé¢å…³é”®è¯åˆ¤æ–­
                        negative_keywords = ['è´Ÿé¢', 'äº‰è®®', 'å†²çª', 'é—®é¢˜', 'é£é™©', 'éšæ‚£', 'æŠ•è¯‰', 'æ‰¹è¯„', 'ä¸æ»¡', 'è´¨ç–‘', 'äº‹æ•…', 'é”™è¯¯']
                        has_negative = any(keyword in reason for keyword in negative_keywords)
                        item['risk_level'] = 4 if has_negative else 2  # æœ‰è´Ÿé¢å…³é”®è¯åˆ™ä¸ºä¸­é£é™©ï¼Œå¦åˆ™ä¸ºä½é£é™©
                    else:
                        # ç¡®ä¿é£é™©ç­‰çº§æ˜¯æ•´æ•°
                        try:
                            item['risk_level'] = int(risk_level)
                        except (ValueError, TypeError):
                            item['risk_level'] = 2  # å¦‚æœæ— æ³•è½¬æ¢ï¼Œé»˜è®¤ä¸ºä½é£é™©
                    
                    # è®¾ç½®å…¶ä»–å¿…è¦å­—æ®µ
                    if 'category' not in item or not item['category']:
                        item['category'] = "æ™®é€šçƒ­ç‚¹"
                    if 'further_investigate' not in item:
                        # åªæœ‰ä¸­é«˜é£é™©æ‰éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥
                        item['further_investigate'] = item['risk_level'] >= 4
                    
                    # ç¡®ä¿ title å­—æ®µå­˜åœ¨
                    if 'topic' in item:
                        item['title'] = item['topic']
                    elif 'title' not in item:
                        item['title'] = topic
                    
                    # è®¾ç½®å…¶ä»–é»˜è®¤å­—æ®µ
                    if 'platform' not in item:
                        item['platform'] = "æŠ–éŸ³"
                    if 'hotness' not in item:
                        item['hotness'] = "ä¸­ç­‰"
                    
                    valid_items.append(item)
                
                # ä½¿ç”¨éªŒè¯åçš„é¡¹ç›®åˆ—è¡¨
                parsed_result['items'] = valid_items
                
                if 'scout_summary' not in parsed_result:
                    # æ·»åŠ é»˜è®¤çš„scout_summary
                    parsed_result['scout_summary'] = {
                        "overall_observation": "æœªç”Ÿæˆç»¼åˆåˆ†æ",
                        "content_summary": "æœªç”Ÿæˆå†…å®¹æ±‡æ€»",
                        "content_analysis": "æœªç”Ÿæˆå†…å®¹åˆ†æ",
                        "potential_risks": ["æœªè¯†åˆ«åˆ°æ˜æ˜¾é£é™©", "å»ºè®®ä¿æŒå¸¸è§„ç›‘æ§", "å…³æ³¨çƒ­ç‚¹åŠ¨æ€å˜åŒ–"],
                        "trend_prediction": "æ— æ³•é¢„æµ‹",
                        "recommendations": ["å»ºè®®ä¿æŒå¸¸è§„ç›‘æ§", "å…³æ³¨çƒ­ç‚¹æ¦œçš„åŠ¨æ€å˜åŒ–", "é‡ç‚¹ç›‘æ§é«˜çƒ­åº¦è¯é¢˜"],
                        "risk_overview": {
                            "high_risk_count": 0,
                            "medium_risk_count": 0,
                            "low_risk_count": 0,
                            "total_count": 0
                        }
                    }
                else:
                    # ç¡®ä¿scout_summaryåŒ…å«æ‰€æœ‰æ–°å­—æ®µ
                    if 'content_summary' not in parsed_result['scout_summary']:
                        parsed_result['scout_summary']['content_summary'] = "æœªç”Ÿæˆå†…å®¹æ±‡æ€»"
                    if 'content_analysis' not in parsed_result['scout_summary']:
                        parsed_result['scout_summary']['content_analysis'] = "æœªç”Ÿæˆå†…å®¹åˆ†æ"
                    if 'potential_risks' not in parsed_result['scout_summary']:
                        parsed_result['scout_summary']['potential_risks'] = ["æœªè¯†åˆ«åˆ°æ˜æ˜¾é£é™©", "å»ºè®®ä¿æŒå¸¸è§„ç›‘æ§", "å…³æ³¨çƒ­ç‚¹åŠ¨æ€å˜åŒ–"]
                    if 'risk_overview' not in parsed_result['scout_summary']:
                        parsed_result['scout_summary']['risk_overview'] = {
                            "high_risk_count": 0,
                            "medium_risk_count": 0,
                            "low_risk_count": 0,
                            "total_count": 0
                        }
                    else:
                        # ç¡®ä¿risk_overviewåŒ…å«total_countå­—æ®µ
                        if 'total_count' not in parsed_result['scout_summary']['risk_overview']:
                            parsed_result['scout_summary']['risk_overview']['total_count'] = 0
                
                # è®¡ç®—é£é™©ç­‰çº§åˆ†å¸ƒ
                high_risk = 0
                medium_risk = 0
                low_risk = 0
                total_count = len(parsed_result['items'])
                
                # é™ä½é£é™©ç­‰çº§åˆ¤æ–­é˜ˆå€¼
                for item in parsed_result['items']:
                    risk_level = item.get('risk_level', 2)
                    if risk_level >= 6:  # é™ä½é«˜é£é™©é˜ˆå€¼
                        high_risk += 1
                    elif risk_level >= 3:  # é™ä½ä¸­é£é™©é˜ˆå€¼
                        medium_risk += 1
                    else:
                        low_risk += 1
                
                # æ›´æ–°risk_overview
                parsed_result['scout_summary']['risk_overview'] = {
                    "high_risk_count": high_risk,
                    "medium_risk_count": medium_risk,
                    "low_risk_count": low_risk,
                    "total_count": total_count
                }
                
                return parsed_result
            else:
                # å°è¯•æ„å»ºå®Œæ•´çš„æŠ¥å‘Šç»“æ„
                if isinstance(parsed_result, list):
                    return {
                        "summary": "ä»çˆ¬å–æ•°æ®ä¸­è¯†åˆ«å‡ºå¤šä¸ªæ½œåœ¨é£é™©è¯é¢˜",
                        "items": parsed_result
                    }
                elif isinstance(parsed_result, dict):
                    # è¡¥å……ç¼ºå¤±çš„å­—æ®µ
                    if 'items' not in parsed_result:
                        parsed_result['items'] = []
                    if 'summary' not in parsed_result:
                        parsed_result['summary'] = "é£é™©åˆ†ææŠ¥å‘Šï¼ˆéƒ¨åˆ†å­—æ®µç¼ºå¤±ï¼‰"
                    return parsed_result
            
        except json.JSONDecodeError:
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
            try:
                # ç§»é™¤å¯èƒ½çš„å‰ç¼€æˆ–åç¼€æ–‡æœ¬
                clean_str = risk_json_str.strip()
                if not clean_str.startswith('{'):
                    clean_str = clean_str[clean_str.find('{'):]
                if not clean_str.endswith('}'):
                    clean_str = clean_str[:clean_str.rfind('}')+1]
                
                # å°è¯•å†æ¬¡è§£æ
                result = json.loads(clean_str)
                return result
            except Exception:
                pass
        except Exception as e:
            # LLMåˆ†æå¤±è´¥æ—¶ï¼Œå°è¯•ä»çˆ¬å–çš„æ•°æ®ä¸­æå–åŸºæœ¬é£é™©é¡¹ç›®
            logger.error(f"LLMåˆ†æå¤±è´¥ï¼Œå°è¯•ä»çˆ¬å–æ•°æ®ä¸­æå–é£é™©é¡¹ç›®: {str(e)}")
            
            try:
                # å°è¯•ç›´æ¥è§£æçˆ¬å–çš„æ•°æ®
                scraped_data = json.loads(scraped_data_json)
                if isinstance(scraped_data, list) and len(scraped_data) > 0:
                    # ä»çˆ¬å–çš„æ•°æ®ä¸­æå–åŸºæœ¬é£é™©é¡¹ç›®
                    risk_items = []
                    for item in scraped_data[:10]:  # åªæå–å‰10ä¸ªé¡¹ç›®
                        risk_item = {
                            "title": item.get('title', item.get('topic', 'æœªçŸ¥è¯é¢˜')),
                            "platform": item.get('platform', 'æœªçŸ¥'),
                            "hotness": item.get('hotness', 'æœªçŸ¥'),
                            "risk_level": 3,  # é»˜è®¤ä¸­é£é™©
                            "category": "æœªåˆ†ç±»",
                            "reason": "LLMåˆ†æå¤±è´¥ï¼Œä»çˆ¬å–æ•°æ®ä¸­æå–",
                            "further_investigate": True
                        }
                        risk_items.append(risk_item)
                    
                    # è¿”å›åŒ…å«æå–çš„é£é™©é¡¹ç›®çš„æŠ¥å‘Š
                    return {
                        "summary": "ä»çˆ¬å–æ•°æ®ä¸­æå–åˆ°é£é™©é¡¹ç›®",
                        "items": risk_items,
                        "scout_summary": {
                            "overall_observation": "LLMåˆ†æå¤±è´¥ï¼Œä»çˆ¬å–æ•°æ®ä¸­æå–é£é™©é¡¹ç›®",
                            "content_summary": f"ä»çˆ¬å–æ•°æ®ä¸­æå–äº† {len(risk_items)} ä¸ªé£é™©é¡¹ç›®",
                            "content_analysis": "LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ·±å…¥åˆ†æ",
                            "potential_risks": ["LLMåˆ†æå¤±è´¥ï¼Œä»çˆ¬å–æ•°æ®ä¸­æå–é£é™©", "å»ºè®®æ£€æŸ¥LLMé…ç½®"],
                            "trend_prediction": "æ— æ³•é¢„æµ‹",
                            "recommendations": ["å»ºè®®æ£€æŸ¥LLMé…ç½®", "ç¡®ä¿APIå¯†é’¥æœ‰æ•ˆ", "æ£€æŸ¥ç½‘ç»œè¿æ¥"],
                            "risk_overview": {
                                "high_risk_count": 0,
                                "medium_risk_count": len(risk_items),
                                "low_risk_count": 0,
                                "total_count": len(risk_items)
                            }
                        }
                    }
            except Exception as parse_error:
                logger.error(f"è§£æçˆ¬å–æ•°æ®å¤±è´¥: {str(parse_error)}")
        
        # LLMåˆ†æå¤±è´¥ä¸”æ— æ³•ä»çˆ¬å–æ•°æ®ä¸­æå–é£é™©é¡¹ç›®æ—¶ï¼Œè¿”å›ç©ºçš„é£é™©é¡¹ç›®åˆ—è¡¨
        logger.error("LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•ä»çˆ¬å–æ•°æ®ä¸­æå–é£é™©é¡¹ç›®ï¼Œè¿”å›ç©ºç»“æœ")
        return {
            "summary": "LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆé£é™©æŠ¥å‘Š",
            "items": [],
            "scout_summary": {
                "overall_observation": "LLMåˆ†æå¤±è´¥",
                "content_summary": "æ— æ³•ç”Ÿæˆå†…å®¹æ±‡æ€»",
                "content_analysis": "æ— æ³•ç”Ÿæˆå†…å®¹åˆ†æ",
                "potential_risks": ["LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•è¯†åˆ«é£é™©", "å»ºè®®æ£€æŸ¥LLMé…ç½®"],
                "trend_prediction": "æ— æ³•é¢„æµ‹",
                "recommendations": ["å»ºè®®æ£€æŸ¥LLMé…ç½®", "ç¡®ä¿APIå¯†é’¥æœ‰æ•ˆ", "æ£€æŸ¥ç½‘ç»œè¿æ¥"],
                "risk_overview": {
                    "high_risk_count": 0,
                    "medium_risk_count": 0,
                    "low_risk_count": 0,
                    "total_count": 0
                }
            }
        }

    def run_once(self) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„ä¾¦å¯Ÿä»»åŠ¡ï¼šçˆ¬å– -> åˆ†æ -> å†™å…¥æƒ…æŠ¥ç«™ã€‚"""
        all_reports = []  # æ”¶é›†æ‰€æœ‰é£é™©æŠ¥å‘Š
        all_risk_items = []  # æ”¶é›†æ‰€æœ‰é£é™©é¡¹ç›®
        llm_failed = False  # æ ‡è®°æ˜¯å¦æœ‰LLMåˆ†æå¤±è´¥
        
        # ç®€åŒ–å¯åŠ¨æ—¥å¿—
        logger.info(f"å¼€å§‹çƒ­ç‚¹ä¾¦å¯Ÿï¼Œå…±æ‰«æ {len(TOPHUB_URLS)} ä¸ªæ¦œå•")
        
        for url in TOPHUB_URLS:
            # 1. çˆ¬å– - ç®€åŒ–è¾“å‡º
            scraped_json = hotlist_crawler(url, save_to_file=True, output_dir=OUTPUT_DIRECTORY, verbose=False)

            if scraped_json:
                # è§£æçˆ¬å–çš„æ•°æ®ï¼Œè·å–æå–çš„æ•°æ®æ•°é‡
                try:
                    scraped_data = json.loads(scraped_json)
                    extracted_count = len(scraped_data)
                    logger.info(f"çˆ¬å– {url} æˆåŠŸï¼Œæå– {extracted_count} æ¡æ•°æ®")
                except:
                    extracted_count = 0
                    logger.warning(f"çˆ¬å– {url} æˆåŠŸï¼Œä½†æ•°æ®æ ¼å¼æ— æ³•è§£æ")
                
                # 2. åˆ†æ - ç°åœ¨è¿”å›å®Œæ•´çš„é£é™©æŠ¥å‘Šå­—å…¸
                risk_report = self._analyze_hotspot(scraped_json)

                if risk_report and isinstance(risk_report, dict):
                    # è·å–æŠ¥å‘Šä¸­çš„é£é™©é¡¹ç›®åˆ—è¡¨
                    risk_items = risk_report.get('items', [])
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰LLMåˆ†æå¤±è´¥
                    if risk_report.get('summary') == "LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆé£é™©æŠ¥å‘Š":
                        llm_failed = True
                    
                    # 3. ä¸ŠæŠ¥æƒ…æŠ¥ (ç”Ÿäº§è€…è¡Œä¸º) - åªä¸ŠæŠ¥itemsåˆ—è¡¨
                    if risk_items:
                        self._append_to_intelligence(risk_items)
                        all_risk_items.extend(risk_items)
                    
                    # 4. æ·»åŠ æ‰€æœ‰æŠ¥å‘Šåˆ°all_reportsï¼ŒåŒ…æ‹¬LLMåˆ†æå¤±è´¥çš„æŠ¥å‘Š
                    all_reports.append(risk_report)
                else:
                    llm_failed = True

                # 5. æ›´æ–° memory (å­˜å‚¨åŸå§‹çˆ¬å–æ•°æ®) - å‡å°‘æ—¥å¿—
                try:
                    data = json.loads(scraped_json)
                    self.memory.extend(data)
                except:
                    continue

        self._save_memory()
        
        # æ‰“å°æ€»é£é™©æ•°é‡
        if all_risk_items:
            try:
                print(f"\n[HotspotHunter] æ€»å…±å‘ç° {len(all_risk_items)} ä¸ªæ½œåœ¨é£é™©")
            except UnicodeEncodeError:
                # é¿å…Windowsç»ˆç«¯ç¼–ç é—®é¢˜
                pass
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Šä½œä¸ºèˆ†æƒ…é¢„è­¦ç»“æœ
        if all_reports:
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŠ¥å‘Šéƒ½å¤±è´¥äº†
            all_failed = all(report.get('summary') == "LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆé£é™©æŠ¥å‘Š" for report in all_reports)
            
            if all_failed:
                # æ‰€æœ‰æŠ¥å‘Šéƒ½å¤±è´¥äº†ï¼Œå°è¯•ä»çˆ¬å–çš„æ•°æ®ä¸­æå–åŸºæœ¬é£é™©é¡¹ç›®
                if all_risk_items:
                    # å³ä½¿LLMåˆ†æå¤±è´¥ï¼Œä¹Ÿè¦ç”ŸæˆåŒ…å«å·²å‘ç°é£é™©é¡¹ç›®çš„æŠ¥å‘Š
                    comprehensive_report = {
                        "summary": "ä»çˆ¬å–æ•°æ®ä¸­æå–åˆ°é£é™©é¡¹ç›®",
                        "topics": all_risk_items,
                        "report_count": len(all_reports),
                        "total_risk_items": len(all_risk_items),
                        "overall_sentiment": {"negative": 0.7, "neutral": 0.2, "positive": 0.1},
                        "risk_signals": ["æ–°å‘ç°çš„é£é™©è¯é¢˜"] if all_risk_items else []
                    }
                    
                    # æ‰“å°æŠ¥å‘Š
                    print(f"\n[HotspotHunter] çƒ­ç‚¹æ¦œåˆ†ææŠ¥å‘Š")
                    print(f"[HotspotHunter] =======================================")
                    print(f"[HotspotHunter] æ‰«æURL: {len(TOPHUB_URLS)} ä¸ª")
                    print(f"[HotspotHunter] å‘ç°é£é™©: {len(all_risk_items)} ä¸ª")
                    print(f"[HotspotHunter] =======================================")
                    
                    return comprehensive_report
                else:
                    # æ‰€æœ‰æŠ¥å‘Šéƒ½å¤±è´¥äº†ï¼Œç”Ÿæˆç®€æ´çš„å¤±è´¥æŠ¥å‘Š
                    failed_report = {
                        "summary": "LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆé£é™©æŠ¥å‘Š",
                        "items": [],
                        "report_count": len(all_reports),
                        "total_risk_items": 0
                    }
                    
                    # æ‰“å°ç®€æ´çš„LLMåˆ†æå¤±è´¥æŠ¥å‘Š
                    print(f"\n[HotspotHunter] çƒ­ç‚¹æ¦œåˆ†ææŠ¥å‘Š")
                    print(f"[HotspotHunter] =======================================")
                    print(f"[HotspotHunter] é”™è¯¯: LLM APIè°ƒç”¨å¤±è´¥ (æ— æ•ˆä»¤ç‰Œ)")
                    print(f"[HotspotHunter] æ‰«æURL: {len(TOPHUB_URLS)} ä¸ª")
                    print(f"[HotspotHunter] å‘ç°é£é™©: {len(all_risk_items)} ä¸ª")
                    print(f"[HotspotHunter] å»ºè®®: æ£€æŸ¥APIå¯†é’¥å’Œé…ç½®")
                    print(f"[HotspotHunter] =======================================")
                    
                    return failed_report
            else:
                # åˆå¹¶æ‰€æœ‰æŠ¥å‘Šçš„æ‘˜è¦
                combined_summary = "\n".join([report.get('summary', '') for report in all_reports if report.get('summary') and report.get('summary') != "LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆé£é™©æŠ¥å‘Š"])
                
                # å¦‚æœæœ‰æˆåŠŸçš„æŠ¥å‘Šï¼Œæ„å»ºç»¼åˆæŠ¥å‘Š
                comprehensive_report = {
                    "summary": combined_summary,
                    "topics": all_risk_items,
                    "report_count": len(all_reports),
                    "total_risk_items": len(all_risk_items),
                    "overall_sentiment": {"negative": 0.7, "neutral": 0.2, "positive": 0.1},
                    "risk_signals": ["æ–°å‘ç°çš„é£é™©è¯é¢˜"] if all_risk_items else []
                }
                
                # åˆå¹¶scout_summaryï¼ˆåªåˆå¹¶æˆåŠŸçš„æŠ¥å‘Šï¼‰
                successful_reports = [report for report in all_reports if report.get('summary') != "LLMåˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆé£é™©æŠ¥å‘Š"]
                if successful_reports:
                    # åˆå¹¶scout_summaryï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªå–ç¬¬ä¸€ä¸ªæˆåŠŸæŠ¥å‘Šçš„scout_summaryï¼‰
                    comprehensive_report['scout_summary'] = successful_reports[0].get('scout_summary', {
                        "overall_observation": "ç»¼åˆåˆ†æå®Œæˆ",
                        "content_summary": "å®Œæˆå†…å®¹æ±‡æ€»",
                        "content_analysis": "å®Œæˆå†…å®¹åˆ†æ",
                        "potential_risks": ["æœªè¯†åˆ«åˆ°æ˜æ˜¾é£é™©", "å»ºè®®ä¿æŒå¸¸è§„ç›‘æ§", "å…³æ³¨çƒ­ç‚¹åŠ¨æ€å˜åŒ–"],
                        "trend_prediction": "çŸ­æœŸå†…èˆ†æƒ…è¶‹åŠ¿å¹³ç¨³",
                        "recommendations": ["å»ºè®®ä¿æŒå¸¸è§„ç›‘æ§é¢‘ç‡", "å…³æ³¨çƒ­ç‚¹æ¦œçš„åŠ¨æ€å˜åŒ–", "é‡ç‚¹ç›‘æ§é«˜çƒ­åº¦è¯é¢˜"],
                        "risk_overview": {
                            "high_risk_count": 0,
                            "medium_risk_count": 0,
                            "low_risk_count": 0,
                            "total_count": 0
                        }
                    })
                
                # å°†æŠ¥å‘Šå†…å®¹æ·»åŠ åˆ°comprehensive_reportä¸­
                comprehensive_report['detailed_report'] = {
                    'scan_url_count': len(TOPHUB_URLS),
                    'total_risk_items': len(all_risk_items),
                    'risk_overview': comprehensive_report['scout_summary'].get('risk_overview', {})
                }
                
                # ä¿å­˜ç»¼åˆæŠ¥å‘Šåˆ°æ–‡ä»¶
                report_file_name = f"hotspot_report_{int(time.time())}_{os.urandom(4).hex()}.json"
                report_file_path = Path(OUTPUT_DIRECTORY) / report_file_name
                with open(report_file_path, 'w', encoding='utf-8') as f:
                    json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)
                
                # æ‰“å°è¯¦ç»†çš„çƒ­ç‚¹æ¦œåˆ†ææŠ¥å‘Š
                print(f"\n[HotspotHunter] çƒ­ç‚¹æ¦œåˆ†ææŠ¥å‘Š")
                print(f"[HotspotHunter] =======================================")
                print(f"[HotspotHunter] æ‰«æURL: {len(TOPHUB_URLS)} ä¸ª")
                print(f"[HotspotHunter] å‘ç°é£é™©: {len(all_risk_items)} ä¸ª")
                
                # æ‰“å°é£é™©ç­‰çº§æ¦‚è§ˆ - è¯¦ç»†ä¿¡æ¯
                if 'scout_summary' in comprehensive_report:
                    risk_overview = comprehensive_report['scout_summary'].get('risk_overview', {})
                    print(f"[HotspotHunter] é£é™©æ¦‚è§ˆ:")
                    print(f"[HotspotHunter]    é«˜é£é™©: {risk_overview.get('high_risk_count', 0)} ä¸ª")
                    print(f"[HotspotHunter]    ä¸­é£é™©: {risk_overview.get('medium_risk_count', 0)} ä¸ª")
                    print(f"[HotspotHunter]    ä½é£é™©: {risk_overview.get('low_risk_count', 0)} ä¸ª")
                
                # æ‰“å°æ¯ä¸ªé£é™©é¡¹ç›®çš„å…³é”®ä¿¡æ¯ - è¯¦ç»†è¾“å‡º
                if all_risk_items:
                    print(f"[HotspotHunter] é£é™©é¡¹ç›® (æ˜¾ç¤ºå‰5ä¸ª):")
                    for i, item in enumerate(all_risk_items[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                        title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
                        risk_level = item.get('risk_level', 'æœªçŸ¥')
                        category = item.get('category', 'æœªåˆ†ç±»')
                        platform = item.get('platform', 'æœªçŸ¥')
                        print(f"[HotspotHunter]    {i+1}. {title} ({risk_level}/10, {category}, {platform})")
                    if len(all_risk_items) > 5:
                        print(f"[HotspotHunter]    ... è¿˜æœ‰ {len(all_risk_items) - 5} ä¸ªé£é™©é¡¹ç›®")
                
                print(f"[HotspotHunter] =======================================")
                
                return comprehensive_report
        else:
            # å¦‚æœæ²¡æœ‰æŠ¥å‘Šï¼Œè¿”å›çœŸå®åæ˜ æƒ…å†µçš„ç»“æœ
            # ä¸å†ç”Ÿæˆè™šå‡çš„å†…å®¹æ±‡æ€»å’Œåˆ†æ
            no_report_result = {
                "summary": "æœªç”Ÿæˆä»»ä½•é£é™©æŠ¥å‘Š",
                "topics": [],
                "report_count": 0,
                "total_risk_items": 0,
                "overall_sentiment": {"negative": 0.7, "neutral": 0.2, "positive": 0.1},
                "risk_signals": []
            }
            
            # æ·»åŠ çœŸå®çš„scout_summaryï¼Œä¸åŒ…å«è™šå‡å†…å®¹
            no_report_result['scout_summary'] = {
                "overall_observation": "æœªç”Ÿæˆä»»ä½•é£é™©æŠ¥å‘Šï¼Œæ— æ³•è¿›è¡Œåˆ†æ",
                "content_summary": "æœªç”Ÿæˆå†…å®¹æ±‡æ€»ï¼Œæ²¡æœ‰å¯ç”¨çš„åˆ†ææ•°æ®",
                "content_analysis": "æœªç”Ÿæˆå†…å®¹åˆ†æï¼Œæ²¡æœ‰å¯ç”¨çš„åˆ†ææ•°æ®",
                "potential_risks": ["æœªç”Ÿæˆä»»ä½•é£é™©æŠ¥å‘Š", "å»ºè®®æ£€æŸ¥ç³»ç»Ÿé…ç½®", "ç¡®ä¿æ‰€æœ‰ç»„ä»¶æ­£å¸¸è¿è¡Œ"],
                "trend_prediction": "æ— æ³•é¢„æµ‹ï¼Œæ²¡æœ‰å¯ç”¨çš„åˆ†ææ•°æ®",
                "recommendations": ["å»ºè®®æ£€æŸ¥ç³»ç»Ÿé…ç½®", "ç¡®ä¿æ‰€æœ‰ç»„ä»¶æ­£å¸¸è¿è¡Œ", "é‡æ–°å¯åŠ¨ç³»ç»Ÿå°è¯•"],
                "risk_overview": {
                    "high_risk_count": 0,
                    "medium_risk_count": 0,
                    "low_risk_count": 0,
                    "total_count": 0
                }
            }
            
            # æ‰“å°ç®€æ´çš„æ— æŠ¥å‘Šç»“æœ
            print(f"\n[HotspotHunter] çƒ­ç‚¹æ¦œåˆ†ææŠ¥å‘Š")
            print(f"[HotspotHunter] =======================================")
            print(f"[HotspotHunter] æ‰«æURL: {len(TOPHUB_URLS)} ä¸ª")
            print(f"[HotspotHunter] å‘ç°é£é™©: 0 ä¸ª")
            print(f"[HotspotHunter] å»ºè®®: æ£€æŸ¥ç³»ç»Ÿé…ç½®")
            print(f"[HotspotHunter] =======================================")
            
            return no_report_result

    def run_loop(self):
        """æŒç»­ä¾¦å¯Ÿå¾ªç¯ã€‚"""
        print(f"Hotspot Hunter ä¾¦å¯Ÿå…µå·²å°±ä½ï¼Œåˆå§‹é¢‘ç‡: {self.interval}s/æ¬¡")
        while True:
            start_time = time.time()
            try:
                self.run_once()
            except Exception as e:
                print(f"[HotspotHunter] å‘ç”Ÿé”™è¯¯: {e}")

            elapsed_time = time.time() - start_time
            sleep_time = max(0, self.interval - elapsed_time)

            print(f"ğŸ’¤ ä¼‘çœ  {sleep_time:.1f}s (å½“å‰è®¾å®šé—´éš”: {self.interval}s)...")
            time.sleep(sleep_time)


# Main entry point for direct execution
if __name__ == "__main__":
    # Create LLM client
    llm_client = LLMClient(LLM_CONFIG)
    
    # Create HotspotHunterAgent instance
    agent = HotspotHunterAgent(llm_client, crawl_interval=HOTSPOT_HUNTER_INTERVAL)
    
    # Run once for testing
    print("ğŸš€ Starting Hotspot Hunter test run...")
    result = agent.run_once()
    print("âœ… Test run completed!")
    
    # Print summary of results
    print(f"ğŸ“Š Results: {result['total_risk_items']} risk items found")
    if result['items']:
        print("ğŸ’¡ Risk items details:")
        for i, item in enumerate(result['items'][:3]):  # Show first 3 items
            print(f"   {i+1}. {item.get('topic', 'Unknown')} (Risk Level: {item.get('risk_level', 'N/A')})")
            print(f"      Reason: {item.get('reason', 'No reason provided')}")
    if result.get('scout_summary'):
        print("ğŸ“‹ Scout Summary:")
        print(f"   Overall: {result['scout_summary'].get('overall_observation', 'No summary')}")
        print(f"   Content: {result['scout_summary'].get('content_summary', 'No content summary')}")