"""
Unified LLM client for the Query Engine, with retry support and smart response parsing.
Compatible with OpenAI, Gemini, v1, and other API formats.
"""

import os
from datetime import datetime
from typing import Any, Dict, Optional, Generator
from loguru import logger

# OpenAI SDK
from openai import OpenAI

# å¼•å…¥é¡¹ç›®é…ç½®
try:
    from ..utils.config import LLM_CONFIG
except ImportError:
    from utils.config import LLM_CONFIG


def with_retry(config=None):
    def decorator(func):
        return func
    return decorator


LLM_RETRY_CONFIG = None


class LLMClient:
    """
    Unified LLM client for multiple API formats.
    Users provide API key, model name, base URL optionally.
    Response parsing is fully automated.
    """

    def __init__(self, api_key: Optional[str] = None, model_name: Optional[str] = None, base_url: Optional[str] = None, **kwargs):
        self.api_key = api_key or LLM_CONFIG.get("api_key") or os.getenv("LLM_API_KEY")
        self.model_name = model_name or LLM_CONFIG.get("model_name") or os.getenv("LLM_MODEL_NAME")
        self.base_url = base_url or LLM_CONFIG.get("base_url") or os.getenv("LLM_API_BASE")
        timeout_default = LLM_CONFIG.get("timeout") or os.getenv("LLM_REQUEST_TIMEOUT") or 1800

        try:
            self.timeout = float(timeout_default)
        except ValueError:
            self.timeout = 1800.0

        if not self.api_key:
            raise ValueError("LLM API key is required.")
        if not self.model_name:
            raise ValueError("LLM model name is required.")

        client_kwargs = {"api_key": self.api_key, "max_retries": 0}
        if self.base_url:
            client_kwargs["base_url"] = self.base_url

        self.client = OpenAI(**client_kwargs)

    # ----------------------- ğŸ”¥ æ–°å¢ï¼šæ—¶é—´å‰ç¼€ -----------------------
    def _prepend_current_time(self, text: str) -> str:
        """ç»™ prompt è‡ªåŠ¨åŠ å½“å‰æ—¶é—´ï¼Œæé«˜è¯­å¢ƒä¸€è‡´æ€§"""
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return f"[å½“å‰æ—¶é—´: {now}]\n{text}"

    # ----------------------- ğŸ”¥ é€šç”¨è°ƒç”¨ -----------------------
    @with_retry(LLM_RETRY_CONFIG)
    def invoke(self, system_prompt: str, user_prompt: str, **kwargs) -> str:
        user_prompt = self._prepend_current_time(user_prompt)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        allowed_keys = {"temperature", "top_p", "presence_penalty", "frequency_penalty", "stream"}
        extra_params = {k: v for k, v in kwargs.items() if k in allowed_keys and v is not None}
        
        # å¤„ç†json_modeå‚æ•°
        json_mode = kwargs.pop("json_mode", False)
        if json_mode:
            # ä½¿ç”¨response_formatå‚æ•°è€Œä¸æ˜¯json_mode
            extra_params["response_format"] = {"type": "json_object"}

        timeout = kwargs.pop("timeout", self.timeout)

        # è®°å½•LLMè¯·æ±‚è¯¦ç»†ä¿¡æ¯
        logger.info(f"[LLM Request] è°ƒç”¨æ¨¡å‹: {self.model_name}")
        logger.info(f"[LLM Request] ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
        logger.info(f"[LLM Request] ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)} å­—ç¬¦")
        logger.info(f"[LLM Request] å‚æ•°: temperature={extra_params.get('temperature', 'default')}, json_mode={json_mode}")

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            timeout=timeout,
            **extra_params,
        )

        # OpenAI SDK: response.choices[0].message.content
        try:
            raw_content = response.choices[0].message.content
            logger.info(f"[LLM Response] æˆåŠŸè·å–å“åº”ï¼Œé•¿åº¦: {len(raw_content)} å­—ç¬¦")
            logger.info(f"[LLM Response] å®Œæ•´å“åº”å†…å®¹: {raw_content}")
        except Exception as e:
            logger.error(f"[LLM Response] è·å–å“åº”å¤±è´¥: {str(e)}")
            raw_content = response

        parsed_content = self.parse_model_response(raw_content)
        return parsed_content

    # ----------------------- ğŸ”¥ æµå¼è°ƒç”¨ -----------------------
    def stream_invoke(self, system_prompt: str, user_prompt: str, **kwargs) -> Generator[str, None, None]:
        user_prompt = self._prepend_current_time(user_prompt)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        extra_params = {k: v for k, v in kwargs.items() if k in {"temperature", "top_p", "presence_penalty", "frequency_penalty"}}
        extra_params["stream"] = True

        timeout = kwargs.pop("timeout", self.timeout)

        try:
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                timeout=timeout,
                **extra_params,
            )

            for chunk in stream:
                if hasattr(chunk, "choices") and chunk.choices:
                    delta = chunk.choices[0].delta
                    if delta and getattr(delta, "content", None):
                        yield delta.content
        except Exception as e:
            logger.error(f"LLM stream request failed: {e}")
            raise e

    def stream_invoke_to_string(self, system_prompt: str, user_prompt: str, **kwargs) -> str:
        byte_chunks = []
        for chunk in self.stream_invoke(system_prompt, user_prompt, **kwargs):
            byte_chunks.append(chunk.encode("utf-8"))
        if byte_chunks:
            return self.parse_model_response(b"".join(byte_chunks).decode("utf-8", errors="replace"))
        return ""

    # ----------------------- ğŸ”¥ æ›´å¼ºçš„æ¨¡å‹è§£æ -----------------------
    @staticmethod
    def parse_model_response(resp: Any) -> str:
        """æ™ºèƒ½è§£æå„ç§å¤§æ¨¡å‹æ ¼å¼"""

        if resp is None:
            return ""

        # string
        if isinstance(resp, str):
            return resp

        # dict æ ¼å¼
        if isinstance(resp, dict):
            # OpenAI
            if "choices" in resp:
                try:
                    return resp["choices"][0]["message"]["content"]
                except:
                    pass

            # Gemini
            if "candidates" in resp:
                try:
                    parts = resp["candidates"][0]["content"]
                    return "".join([p.get("text", "") for p in parts])
                except:
                    pass

            if "output_text" in resp:
                return resp["output_text"]

            return str(resp)

        # openai.ChatCompletionObject
        if hasattr(resp, "choices"):
            try:
                return resp.choices[0].message.content
            except:
                pass

        # openai.ChatCompletionMessage
        if hasattr(resp, "content"):
            return resp.content

        return str(resp)
