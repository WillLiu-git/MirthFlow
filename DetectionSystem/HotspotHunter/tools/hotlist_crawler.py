# tools/hotspot_scraper.py

import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import json
import os
from datetime import datetime
import sys
from pathlib import Path

# å¯¼å…¥é¡¹ç›®é…ç½® - æ”¯æŒç›¸å¯¹å’Œç»å¯¹å¯¼å…¥
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..utils.config import TOPHUB_URLS, REQUEST_HEADERS, TITLE_LINK_SELECTOR, HOTNESS_SELECTOR
except ImportError:
    # å›é€€åˆ°ç»å¯¹å¯¼å…¥
    from utils.config import TOPHUB_URLS, REQUEST_HEADERS, TITLE_LINK_SELECTOR, HOTNESS_SELECTOR


def hotlist_crawler(
        url: str,
        save_to_file: bool = False,
        output_dir: str = 'scraped_hot_lists_json',
        verbose: bool = True
) -> str | None:
    """
    çˆ¬å– Tophub æ¦œå•æ•°æ®ï¼Œè¿”å› JSON å­—ç¬¦ä¸²å¹¶å¯é€‰æ‹©ä¿å­˜ä¸ºæ–‡ä»¶ã€‚

    æ¯æ¡æ•°æ®æ–°å¢å­—æ®µï¼š
        scraped_at = å½“å‰çˆ¬å–æ—¶é—´
    """

    board_id = url.split('/')[-1]

    # è®°å½•å½“å‰æ—¶é—´
    scraped_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # æ¸©å’Œä¸€ç‚¹ï¼Œå‡å°‘è¢« Ban
    time.sleep(2)

    try:
        # 1. è·å–ç½‘é¡µå†…å®¹
        response = requests.get(url, headers=REQUEST_HEADERS, timeout=20)
        response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ

        # 2. è§£æç½‘é¡µå†…å®¹
        soup = BeautifulSoup(response.text, "html.parser")
        hot_topics_data = []

        # æŸ¥æ‰¾æ‰€æœ‰çƒ­é—¨è¯é¢˜æ¡ç›®
        items = soup.select("tr")

        # æ‰“å°çˆ¬å–URLå’ŒIDï¼ˆä»…å½“verboseä¸ºTrueæ—¶ï¼‰
        if verbose:
            print(f"[Tool] å¼€å§‹çˆ¬å– URL: {url} (ID: {board_id})")

        # è·³è¿‡è¡¨å¤´ï¼Œä»ç¬¬2è¡Œå¼€å§‹
        for index, item in enumerate(items[1:], start=0):
            # æ’å
            rank_cell = item.select_one("td:nth-child(1)")
            rank = rank_cell.get_text(strip=True).replace('.', '') if rank_cell else str(index + 1)

            # æ ‡é¢˜ + é“¾æ¥
            title_link_tag = item.select_one(TITLE_LINK_SELECTOR)
            if title_link_tag:
                title = title_link_tag.get_text(strip=True)
                link = title_link_tag.get("href")
            else:
                title = "N/A"
                link = "N/A"

            # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œå°è¯•å®¹é”™
            if title == "N/A":
                second_td = item.select_one("td:nth-child(2)")
                if second_td:
                    title = second_td.get_text(strip=True)

            # çƒ­åº¦
            hot_tag = item.select_one(HOTNESS_SELECTOR)
            hotness = hot_tag.get_text(strip=True) if hot_tag and hot_tag.text.strip() else "0"

            hot_topics_data.append({
                "rank": rank,
                "title": title,
                "hotness": hotness,
                "link": link,
                "source_url": url,
                "scraped_at": scraped_at   # æ—¶é—´å­—æ®µ
            })

        # 4. è½¬ä¸º JSON å­—ç¬¦ä¸²
        df = pd.DataFrame(hot_topics_data)
        json_result = df.to_json(orient="records", force_ascii=False)

        # 5. ä¿å­˜æ–‡ä»¶
        if save_to_file:
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, f"tophub_{board_id}.json")

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(hot_topics_data, f, ensure_ascii=False, indent=4)

        # ä»…å½“verboseä¸ºTrueæ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯
        if verbose:
            print(f" [Tool] æå–æˆåŠŸï¼Œå…± {len(hot_topics_data)} æ¡æ•°æ®ã€‚")
            if save_to_file:
                print(f"[Tool] æ•°æ®å·²ä¿å­˜è‡³: {output_path}")

        return json_result

    except requests.exceptions.RequestException as e:
        print(f"[Tool] è¯·æ±‚é”™è¯¯: {e}")
        return None


# --------------------------------------------------
if __name__ == "__main__":

    OUTPUT_DIR = "scraped_hot_lists_json"
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ä½ è¦çˆ¬å–çš„æ¦œå•ï¼ˆå¯è‡ªè¡Œæ‰©å±•ï¼‰
    urls = TOPHUB_URLS

    print(f"â­ æœ¬æ¬¡å°†çˆ¬å– {len(urls)} ä»½æ¦œå•...\n")

    all_results = {}

    for url in urls:
        json_data = hotlist_crawler(
            url,
            save_to_file=True,  # è‡ªåŠ¨ä¿å­˜
            output_dir=OUTPUT_DIR
        )

        if json_data:
            board_id = url.split('/')[-1]
            all_results[board_id] = json_data

    print("\nğŸ‰ å…¨éƒ¨çˆ¬å–å®Œæˆï¼")
    print("ğŸ“Š å·²è·å–ç»“æœï¼š", list(all_results.keys()))
