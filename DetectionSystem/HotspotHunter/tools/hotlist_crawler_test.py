# hotspot_scraper_test.py â€”â€” ä¸€ä½“åŒ–æµ‹è¯•ç‰ˆ
# âœ” æ— å¤–éƒ¨ä¾èµ–é¡¹
# âœ” æ”¯æŒå¤šä¸ªæ¦œå•
# âœ” è‡ªåŠ¨ä¿å­˜ JSON æ–‡ä»¶
# âœ” è‡ªåŠ¨å¸¦ scraped_at æ—¶é—´æˆ³

import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import json
import os
from datetime import datetime

# --------------------------------------------------
# å…¨å±€è¯·æ±‚ headersï¼ˆæ¨¡æ‹Ÿå¸¸è§„æµè§ˆå™¨ï¼‰
# --------------------------------------------------
REQUEST_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml",
    "Accept-Language": "zh-CN,zh;q=0.9",
}

# CSS é€‰æ‹©å™¨ â€”â€” é’ˆå¯¹ TopHub é€šç”¨æ¦œå•
TITLE_LINK_SELECTOR = "td:nth-child(2) a"
HOTNESS_SELECTOR = "td:nth-child(3)"


# --------------------------------------------------
# æ ¸å¿ƒçˆ¬è™«å‡½æ•°
# --------------------------------------------------
def scrape_tophub_hot_list(url: str, save_to_file: bool = False,
                           output_dir: str = 'scraped_hot_lists_json') -> str | None:
    """
    çˆ¬å–å•ä¸ª Tophub æ¦œå•å¹¶è¿”å› JSON å­—ç¬¦ä¸²
    """
    board_id = url.split('/')[-1]
    print(f"\nğŸš€ [Scraper] å¼€å§‹çˆ¬å– {board_id} ...")

    scraped_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    time.sleep(2)  # é˜²æ­¢è®¿é—®è¿‡å¿«è¢« Ban

    try:
        response = requests.get(url, headers=REQUEST_HEADERS, timeout=15)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        rows = soup.select("table.table tbody tr")

        if not rows:
            print("âš ï¸ æœªè§£æåˆ°æ¦œå•æ•°æ®ï¼")
            return None

        items = []

        for idx, row in enumerate(rows):
            # æ’å
            rank_tag = row.select_one("td:nth-child(1)")
            rank = rank_tag.text.strip().replace('.', '') if rank_tag else str(idx + 1)

            # æ ‡é¢˜ + é“¾æ¥
            title_tag = row.select_one(TITLE_LINK_SELECTOR)
            if title_tag:
                title = title_tag.text.strip()
                link = title_tag.get("href")
            else:
                title = "N/A"
                link = "N/A"

            # çƒ­åº¦
            hot_tag = row.select_one(HOTNESS_SELECTOR)
            hotness = hot_tag.text.strip() if hot_tag and hot_tag.text.strip() else "0"

            items.append({
                "rank": rank,
                "title": title,
                "hotness": hotness,
                "link": link,
                "source_url": url,
                "scraped_at": scraped_at
            })

        print(f"âœ” [Scraper] {board_id} æå–æˆåŠŸï¼š{len(items)} æ¡æ•°æ®")

        # ç”Ÿæˆ JSON å­—ç¬¦ä¸²
        df = pd.DataFrame(items)
        json_str = df.to_json(orient="records", force_ascii=False)

        # æ˜¯å¦å†™å…¥æ–‡ä»¶
        if save_to_file:
            os.makedirs(output_dir, exist_ok=True)
            filepath = os.path.join(output_dir, f"tophub_{board_id}.json")

            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(items, f, ensure_ascii=False, indent=4)

            print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")

        return json_str

    except Exception as e:
        print(f"âŒ [Scraper] è¯·æ±‚å¤±è´¥: {e}")
        return None


# --------------------------------------------------
# ä¸»ç¨‹åºï¼šè‡ªåŠ¨çˆ¬å–å¤šä¸ªæ¦œå•
# --------------------------------------------------
if __name__ == "__main__":

    OUTPUT_DIR = "scraped_hot_lists_json"
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ä½ è¦çˆ¬å–çš„æ¦œå•ï¼ˆå¯è‡ªè¡Œæ‰©å±•ï¼‰
    urls = [
        "https://tophub.today/n/K7GdaMgdQy",  # æŠ–éŸ³
        "https://tophub.today/n/KqndgxeLl9",  # å¾®åš
        "https://tophub.today/n/rx9oz6oXbq",  # çŸ¥ä¹
    ]

    print(f"â­ æœ¬æ¬¡å°†çˆ¬å– {len(urls)} ä»½æ¦œå•...\n")

    all_results = {}

    for url in urls:
        json_data = scrape_tophub_hot_list(
            url,
            save_to_file=True,  # è‡ªåŠ¨ä¿å­˜
            output_dir=OUTPUT_DIR
        )

        if json_data:
            board_id = url.split('/')[-1]
            all_results[board_id] = json_data

    print("\nğŸ‰ å…¨éƒ¨çˆ¬å–å®Œæˆï¼")
    print("ğŸ“Š å·²è·å–ç»“æœï¼š", list(all_results.keys()))
